{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IVUL: WITH CLASS BALANCING (CWE 119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 29313, 1: 10440}\n",
      "Train split before balancing: {0: 23450, 1: 8352}\n",
      "Balanced training class distribution: {0: 8352, 1: 8352}\n",
      "Final training class distribution: {0: 8352, 1: 8352}\n",
      "Training for 15 epochs...\n",
      "Epoch 1/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 191ms/step - accuracy: 0.6043 - loss: 0.6533\n",
      "Epoch 2/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 189ms/step - accuracy: 0.6288 - loss: 0.6249\n",
      "Epoch 3/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 188ms/step - accuracy: 0.7085 - loss: 0.5527\n",
      "Epoch 4/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 187ms/step - accuracy: 0.7658 - loss: 0.4816\n",
      "Epoch 5/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 189ms/step - accuracy: 0.7843 - loss: 0.4436\n",
      "Epoch 6/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 188ms/step - accuracy: 0.8053 - loss: 0.4126\n",
      "Epoch 7/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 187ms/step - accuracy: 0.8183 - loss: 0.3903\n",
      "Epoch 8/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 188ms/step - accuracy: 0.8232 - loss: 0.3701\n",
      "Epoch 9/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 187ms/step - accuracy: 0.8324 - loss: 0.3515\n",
      "Epoch 10/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 188ms/step - accuracy: 0.8458 - loss: 0.3366\n",
      "Epoch 11/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m99s\u001b[0m 189ms/step - accuracy: 0.8547 - loss: 0.3169\n",
      "Epoch 12/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 188ms/step - accuracy: 0.8612 - loss: 0.3104\n",
      "Epoch 13/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 189ms/step - accuracy: 0.8633 - loss: 0.2962\n",
      "Epoch 14/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m98s\u001b[0m 188ms/step - accuracy: 0.8747 - loss: 0.2806\n",
      "Epoch 15/15\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m97s\u001b[0m 186ms/step - accuracy: 0.8802 - loss: 0.2672\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 54ms/step - accuracy: 0.7759 - loss: 0.4558\n",
      "\n",
      "Final Test Accuracy: 0.7802\n",
      "\n",
      "Computing additional metrics...\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 50ms/step\n",
      "Accuracy        : 0.7802\n",
      "True Pos Rate   : 0.8333\n",
      "False Neg Rate  : 0.1667\n",
      "False Pos Rate  : 0.2388\n",
      "Precision       : 0.5541\n",
      "F1 Score        : 0.6656\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
    "from math import floor, sqrt\n",
    "\n",
    "# ---------------------------------------\n",
    "# 0. Reproducibility Setup\n",
    "# ---------------------------------------\n",
    "SEED = 41\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Data Loading + Preprocessing\n",
    "# ---------------------------------------\n",
    "def parse_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []\n",
    "        label = None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped and gadget and label in [0, 1]:\n",
    "                yield \"\\n\".join(gadget), label\n",
    "                gadget = []\n",
    "                label = None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    label_candidate = int(stripped)\n",
    "                    if label_candidate in [0, 1]:\n",
    "                        label = label_candidate\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "\n",
    "def code_to_image(code_sample):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "    padded = np.zeros((224, 224), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    padded[:h, :w] = cropped\n",
    "    return padded\n",
    "\n",
    "def load_balanced_data(filepath, seed=SEED):\n",
    "    codes, labels = zip(*parse_file(filepath))\n",
    "    labels = np.array(labels).astype(np.int32)\n",
    "\n",
    "    # Show original distribution\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"Original dataset class distribution: {dict(zip(unique, counts))}\")\n",
    "\n",
    "    # Train-test split\n",
    "    X_train_codes, X_test_codes, y_train_all, y_test = train_test_split(\n",
    "        codes, labels, test_size=0.2, stratify=labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Before balancing training\n",
    "    unique_train_pre, count_train_pre = np.unique(y_train_all, return_counts=True)\n",
    "    print(f\"Train split before balancing: {dict(zip(unique_train_pre, count_train_pre))}\")\n",
    "\n",
    "    # Balance training set (undersample class 0)\n",
    "    y_train_all = np.array(y_train_all)\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    neg_sample = rng.choice(neg_idx, len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    # Balanced training set\n",
    "    balanced_codes = [X_train_codes[i] for i in balanced_idx]\n",
    "    y_train = y_train_all[balanced_idx]\n",
    "\n",
    "    unique_train_post, count_train_post = np.unique(y_train, return_counts=True)\n",
    "    print(f\"Balanced training class distribution: {dict(zip(unique_train_post, count_train_post))}\")\n",
    "\n",
    "    # Convert to images\n",
    "    X_train = np.array([code_to_image(code) for code in balanced_codes], dtype=np.float32) / 255.0\n",
    "    X_train = np.expand_dims(X_train, -1)\n",
    "\n",
    "    # Test set\n",
    "    X_test = np.array([code_to_image(code) for code in X_test_codes], dtype=np.float32) / 255.0\n",
    "    X_test = np.expand_dims(X_test, -1)\n",
    "    y_test = np.array(y_test).astype(np.int32)\n",
    "\n",
    "    # Final distributions\n",
    "    print(f\"Final training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test), class_weight_dict\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Model Training Using Given Parameters\n",
    "# ---------------------------------------\n",
    "def train_final_model(filepath):\n",
    "    (X_train, X_test, y_train, y_test), class_weight = load_balanced_data(filepath)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    dropout = 0.4\n",
    "    dense_units = 128\n",
    "    batch_size = 32\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(224, 224, 1)),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Dropout(dropout),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation=\"relu\"),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(\"Training for 15 epochs...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=15,\n",
    "        class_weight=class_weight,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nComputing additional metrics...\")\n",
    "    y_pred_probs = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy        : {accuracy:.4f}\")\n",
    "    print(f\"True Pos Rate   : {tpr:.4f}\")\n",
    "    print(f\"False Neg Rate  : {fnr:.4f}\")\n",
    "    print(f\"False Pos Rate  : {fpr:.4f}\")\n",
    "    print(f\"Precision       : {precision:.4f}\")\n",
    "    print(f\"F1 Score        : {f1:.4f}\")\n",
    "\n",
    "# --- Run it ---\n",
    "train_final_model(\"cwe119_cgd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IVUL: WITH CLASS BALANCING (CWE 399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 14600, 1: 7285}\n",
      "Train split before balancing: {0: 11680, 1: 5828}\n",
      "Balanced training class distribution: {0: 5828, 1: 5828}\n",
      "Final training class distribution: {0: 5828, 1: 5828}\n",
      "Training for 15 epochs...\n",
      "Epoch 1/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 189ms/step - accuracy: 0.5515 - loss: 0.6793\n",
      "Epoch 2/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 187ms/step - accuracy: 0.6657 - loss: 0.5915\n",
      "Epoch 3/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 187ms/step - accuracy: 0.7792 - loss: 0.4306\n",
      "Epoch 4/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 187ms/step - accuracy: 0.8227 - loss: 0.3561\n",
      "Epoch 5/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 189ms/step - accuracy: 0.8496 - loss: 0.3143\n",
      "Epoch 6/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 187ms/step - accuracy: 0.8671 - loss: 0.2842\n",
      "Epoch 7/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 187ms/step - accuracy: 0.8905 - loss: 0.2461\n",
      "Epoch 8/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 187ms/step - accuracy: 0.9041 - loss: 0.2196\n",
      "Epoch 9/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 186ms/step - accuracy: 0.9078 - loss: 0.2046\n",
      "Epoch 10/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 186ms/step - accuracy: 0.9178 - loss: 0.1865\n",
      "Epoch 11/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m68s\u001b[0m 186ms/step - accuracy: 0.9248 - loss: 0.1685\n",
      "Epoch 12/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 195ms/step - accuracy: 0.9326 - loss: 0.1583\n",
      "Epoch 13/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 194ms/step - accuracy: 0.9354 - loss: 0.1505\n",
      "Epoch 14/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m70s\u001b[0m 193ms/step - accuracy: 0.9433 - loss: 0.1339\n",
      "Epoch 15/15\n",
      "\u001b[1m365/365\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m71s\u001b[0m 194ms/step - accuracy: 0.9500 - loss: 0.1258\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step - accuracy: 0.8897 - loss: 0.2935\n",
      "\n",
      "Final Test Accuracy: 0.8773\n",
      "\n",
      "Computing additional metrics...\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 56ms/step\n",
      "Accuracy        : 0.8773\n",
      "True Pos Rate   : 0.9183\n",
      "False Neg Rate  : 0.0817\n",
      "False Pos Rate  : 0.1432\n",
      "Precision       : 0.7620\n",
      "F1 Score        : 0.8329\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
    "from math import floor, sqrt\n",
    "\n",
    "# ---------------------------------------\n",
    "# 0. Reproducibility Setup\n",
    "# ---------------------------------------\n",
    "SEED = 41\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Data Loading + Preprocessing\n",
    "# ---------------------------------------\n",
    "def parse_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []\n",
    "        label = None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped and gadget and label in [0, 1]:\n",
    "                yield \"\\n\".join(gadget), label\n",
    "                gadget = []\n",
    "                label = None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    label_candidate = int(stripped)\n",
    "                    if label_candidate in [0, 1]:\n",
    "                        label = label_candidate\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "\n",
    "def code_to_image(code_sample):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "    padded = np.zeros((224, 224), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    padded[:h, :w] = cropped\n",
    "    return padded\n",
    "\n",
    "def load_balanced_data(filepath, seed=SEED):\n",
    "    codes, labels = zip(*parse_file(filepath))\n",
    "    labels = np.array(labels).astype(np.int32)\n",
    "\n",
    "    # Show original distribution\n",
    "    unique, counts = np.unique(labels, return_counts=True)\n",
    "    print(f\"Original dataset class distribution: {dict(zip(unique, counts))}\")\n",
    "\n",
    "    # Train-test split\n",
    "    X_train_codes, X_test_codes, y_train_all, y_test = train_test_split(\n",
    "        codes, labels, test_size=0.2, stratify=labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Before balancing training\n",
    "    unique_train_pre, count_train_pre = np.unique(y_train_all, return_counts=True)\n",
    "    print(f\"Train split before balancing: {dict(zip(unique_train_pre, count_train_pre))}\")\n",
    "\n",
    "    # Balance training set (undersample class 0)\n",
    "    y_train_all = np.array(y_train_all)\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    neg_sample = rng.choice(neg_idx, len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    # Balanced training set\n",
    "    balanced_codes = [X_train_codes[i] for i in balanced_idx]\n",
    "    y_train = y_train_all[balanced_idx]\n",
    "\n",
    "    unique_train_post, count_train_post = np.unique(y_train, return_counts=True)\n",
    "    print(f\"Balanced training class distribution: {dict(zip(unique_train_post, count_train_post))}\")\n",
    "\n",
    "    # Convert to images\n",
    "    X_train = np.array([code_to_image(code) for code in balanced_codes], dtype=np.float32) / 255.0\n",
    "    X_train = np.expand_dims(X_train, -1)\n",
    "\n",
    "    # Test set\n",
    "    X_test = np.array([code_to_image(code) for code in X_test_codes], dtype=np.float32) / 255.0\n",
    "    X_test = np.expand_dims(X_test, -1)\n",
    "    y_test = np.array(y_test).astype(np.int32)\n",
    "\n",
    "    # Final distributions\n",
    "    print(f\"Final training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test), class_weight_dict\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Model Training Using Given Parameters\n",
    "# ---------------------------------------\n",
    "def train_final_model(filepath):\n",
    "    (X_train, X_test, y_train, y_test), class_weight = load_balanced_data(filepath)\n",
    "\n",
    "    learning_rate = 0.001\n",
    "    dropout = 0.4\n",
    "    dense_units = 128\n",
    "    batch_size = 32\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(224, 224, 1)),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Dropout(dropout),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation=\"relu\"),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(\"Training for 15 epochs...\")\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=batch_size,\n",
    "        epochs=15,\n",
    "        class_weight=class_weight,\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    loss, acc = model.evaluate(X_test, y_test)\n",
    "    print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nComputing additional metrics...\")\n",
    "    y_pred_probs = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy        : {accuracy:.4f}\")\n",
    "    print(f\"True Pos Rate   : {tpr:.4f}\")\n",
    "    print(f\"False Neg Rate  : {fnr:.4f}\")\n",
    "    print(f\"False Pos Rate  : {fpr:.4f}\")\n",
    "    print(f\"Precision       : {precision:.4f}\")\n",
    "    print(f\"F1 Score        : {f1:.4f}\")\n",
    "\n",
    "# --- Run it ---\n",
    "train_final_model(\"cwe399_cgd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING (CWE 119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-23 23:06:02,979] A new study created in memory with name: no-name-a5da2157-5aed-4958-9136-15ce0f950a2e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean fraction of data preserved with TARGET_SIZE=32: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-23 23:06:28,221] Trial 0 finished with value: 0.5404929577464789 and parameters: {'learning_rate': 1.911834782609028e-05, 'dropout': 0.4415879914992911, 'dense_units': 64, 'batch_size': 64}. Best is trial 0 with value: 0.5404929577464789.\n",
      "[I 2025-05-23 23:06:50,459] Trial 1 finished with value: 0.596327967806841 and parameters: {'learning_rate': 4.729700700417656e-05, 'dropout': 0.3798021638730412, 'dense_units': 256, 'batch_size': 128}. Best is trial 1 with value: 0.596327967806841.\n",
      "[I 2025-05-23 23:07:20,370] Trial 2 finished with value: 0.5404929577464789 and parameters: {'learning_rate': 2.145725433638952e-05, 'dropout': 0.4709343571294727, 'dense_units': 64, 'batch_size': 32}. Best is trial 1 with value: 0.596327967806841.\n",
      "[I 2025-05-23 23:08:02,175] Trial 3 finished with value: 0.7376760563380281 and parameters: {'learning_rate': 0.0029901502741362805, 'dropout': 0.36190349195732446, 'dense_units': 64, 'batch_size': 16}. Best is trial 3 with value: 0.7376760563380281.\n",
      "[I 2025-05-23 23:08:27,220] Trial 4 finished with value: 0.5377263581488934 and parameters: {'learning_rate': 2.108636121064615e-05, 'dropout': 0.48874400024034503, 'dense_units': 128, 'batch_size': 64}. Best is trial 3 with value: 0.7376760563380281.\n",
      "[I 2025-05-23 23:08:49,575] Trial 5 finished with value: 0.5988430583501007 and parameters: {'learning_rate': 6.900255242587715e-05, 'dropout': 0.20755844977043655, 'dense_units': 256, 'batch_size': 128}. Best is trial 3 with value: 0.7376760563380281.\n",
      "[I 2025-05-23 23:09:19,477] Trial 6 finished with value: 0.692907444668008 and parameters: {'learning_rate': 0.00018395431877188623, 'dropout': 0.4843966818656244, 'dense_units': 64, 'batch_size': 32}. Best is trial 3 with value: 0.7376760563380281.\n",
      "[I 2025-05-23 23:10:03,920] Trial 7 finished with value: 0.7266096579476862 and parameters: {'learning_rate': 0.0003454153585302207, 'dropout': 0.3600507090834878, 'dense_units': 128, 'batch_size': 16}. Best is trial 3 with value: 0.7376760563380281.\n",
      "[I 2025-05-23 23:10:29,330] Trial 8 finished with value: 0.5379778672032193 and parameters: {'learning_rate': 1.6859964121242337e-05, 'dropout': 0.275093166912519, 'dense_units': 256, 'batch_size': 64}. Best is trial 3 with value: 0.7376760563380281.\n",
      "[I 2025-05-23 23:11:13,601] Trial 9 finished with value: 0.6270120724346077 and parameters: {'learning_rate': 0.004760886514111258, 'dropout': 0.47841562768587015, 'dense_units': 128, 'batch_size': 16}. Best is trial 3 with value: 0.7376760563380281.\n",
      "[I 2025-05-23 23:12:06,965] Trial 10 finished with value: 0.6413480885311871 and parameters: {'learning_rate': 0.004106732763675897, 'dropout': 0.29975442623009324, 'dense_units': 512, 'batch_size': 16}. Best is trial 3 with value: 0.7376760563380281.\n",
      "[I 2025-05-23 23:12:51,666] Trial 11 finished with value: 0.7522635814889336 and parameters: {'learning_rate': 0.000980425519311169, 'dropout': 0.38962491219562356, 'dense_units': 128, 'batch_size': 16}. Best is trial 11 with value: 0.7522635814889336.\n",
      "[I 2025-05-23 23:13:45,164] Trial 12 finished with value: 0.7432092555331992 and parameters: {'learning_rate': 0.0012242345355157399, 'dropout': 0.40164098161353023, 'dense_units': 512, 'batch_size': 16}. Best is trial 11 with value: 0.7522635814889336.\n",
      "[I 2025-05-23 23:14:39,580] Trial 13 finished with value: 0.7562877263581489 and parameters: {'learning_rate': 0.0010315999224959939, 'dropout': 0.4126873240095259, 'dense_units': 512, 'batch_size': 16}. Best is trial 13 with value: 0.7562877263581489.\n",
      "[I 2025-05-23 23:15:33,609] Trial 14 finished with value: 0.7623239436619719 and parameters: {'learning_rate': 0.0008073482804453021, 'dropout': 0.41562997944242025, 'dense_units': 512, 'batch_size': 16}. Best is trial 14 with value: 0.7623239436619719.\n",
      "[I 2025-05-23 23:16:26,681] Trial 15 finished with value: 0.7600603621730382 and parameters: {'learning_rate': 0.0008141681617087885, 'dropout': 0.4402000579471047, 'dense_units': 512, 'batch_size': 16}. Best is trial 14 with value: 0.7623239436619719.\n",
      "[I 2025-05-23 23:17:20,731] Trial 16 finished with value: 0.7628269617706237 and parameters: {'learning_rate': 0.0003442842981527378, 'dropout': 0.4350711297657903, 'dense_units': 512, 'batch_size': 16}. Best is trial 16 with value: 0.7628269617706237.\n",
      "[I 2025-05-23 23:17:44,593] Trial 17 finished with value: 0.727112676056338 and parameters: {'learning_rate': 0.0003114787302280175, 'dropout': 0.3286336324916885, 'dense_units': 512, 'batch_size': 128}. Best is trial 16 with value: 0.7628269617706237.\n",
      "[I 2025-05-23 23:18:21,098] Trial 18 finished with value: 0.6949195171026157 and parameters: {'learning_rate': 0.00013868421992328775, 'dropout': 0.43689326132862577, 'dense_units': 512, 'batch_size': 32}. Best is trial 16 with value: 0.7628269617706237.\n",
      "[I 2025-05-23 23:19:14,964] Trial 19 finished with value: 0.7638329979879276 and parameters: {'learning_rate': 0.00041314272442970695, 'dropout': 0.32317539094635517, 'dense_units': 512, 'batch_size': 16}. Best is trial 19 with value: 0.7638329979879276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Trial:\n",
      "Accuracy: 0.7638\n",
      "  learning_rate: 0.00041314272442970695\n",
      "  dropout: 0.32317539094635517\n",
      "  dense_units: 512\n",
      "  batch_size: 16\n",
      "\n",
      "Training final model on train + val...\n",
      "Epoch 1/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.5972 - loss: 0.6558\n",
      "Epoch 2/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.6458 - loss: 0.6155\n",
      "Epoch 3/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7179 - loss: 0.5451\n",
      "Epoch 4/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7662 - loss: 0.4762\n",
      "Epoch 5/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7965 - loss: 0.4355\n",
      "Epoch 6/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8015 - loss: 0.4098\n",
      "Epoch 7/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8158 - loss: 0.3857\n",
      "Epoch 8/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8295 - loss: 0.3668\n",
      "Epoch 9/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8398 - loss: 0.3388\n",
      "Epoch 10/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.8508 - loss: 0.3261\n",
      "Epoch 11/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8560 - loss: 0.3077\n",
      "Epoch 12/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8628 - loss: 0.2958\n",
      "Epoch 13/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8693 - loss: 0.2793\n",
      "Epoch 14/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8792 - loss: 0.2715\n",
      "Epoch 15/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8886 - loss: 0.2537\n",
      "Epoch 16/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8962 - loss: 0.2343\n",
      "Epoch 17/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8977 - loss: 0.2292\n",
      "Epoch 18/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9035 - loss: 0.2217\n",
      "Epoch 19/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9071 - loss: 0.2194\n",
      "Epoch 20/20\n",
      "\u001b[1m914/914\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9096 - loss: 0.2080\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Accuracy        : 0.7727\n",
      "True Pos Rate   : 0.8817\n",
      "False Neg Rate  : 0.1183\n",
      "False Pos Rate  : 0.2661\n",
      "Precision       : 0.5413\n",
      "F1 Score        : 0.6708\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
    "from math import floor, sqrt\n",
    "import optuna\n",
    "\n",
    "SEED = 41\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "TARGET_SIZE = 32\n",
    "\n",
    "def compute_mean_coverage(filepath, target_size):\n",
    "    lengths = []\n",
    "    with open(filepath, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []\n",
    "        label = None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if gadget and label in [0, 1]:\n",
    "                    byte_array = bytearray(\"\\n\".join(gadget), 'utf-8')\n",
    "                    lengths.append(len(byte_array))\n",
    "                gadget = []\n",
    "                label = None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    label_candidate = int(stripped)\n",
    "                    if label_candidate in [0, 1]:\n",
    "                        label = label_candidate\n",
    "                    else:\n",
    "                        gadget = []\n",
    "                        label = None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "\n",
    "    max_pixels = target_size * target_size\n",
    "    coverage = [min(floor(sqrt(l))**2, max_pixels) / l for l in lengths if l > 0]\n",
    "    mean_coverage = np.mean(coverage)\n",
    "    print(f\"\\nMean fraction of data preserved with TARGET_SIZE={target_size}: {mean_coverage:.4f}\")\n",
    "    return mean_coverage\n",
    "\n",
    "def parse_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []\n",
    "        label = None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if gadget and label in [0, 1]:\n",
    "                    yield \"\\n\".join(gadget), label\n",
    "                gadget = []\n",
    "                label = None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    label_candidate = int(stripped)\n",
    "                    if label_candidate in [0, 1]:\n",
    "                        label = label_candidate\n",
    "                    else:\n",
    "                        gadget = []\n",
    "                        label = None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "\n",
    "def code_to_image(code_sample):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "    padded = np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    h = min(h, TARGET_SIZE)\n",
    "    w = min(w, TARGET_SIZE)\n",
    "    padded[:h, :w] = cropped[:h, :w]\n",
    "    return padded\n",
    "\n",
    "def load_balanced_data(filepath, seed=SEED):\n",
    "    codes, labels = zip(*parse_file(filepath))\n",
    "    labels = np.array(labels).astype(np.int32)\n",
    "\n",
    "    codes_trainval, codes_test, y_trainval_raw, y_test = train_test_split(\n",
    "        codes, labels, test_size=0.2, stratify=labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    codes_train, codes_val, y_train_raw, y_val = train_test_split(\n",
    "        codes_trainval, y_trainval_raw, test_size=0.125, stratify=y_trainval_raw, random_state=seed\n",
    "    )\n",
    "\n",
    "    y_train_raw = np.array(y_train_raw)\n",
    "    pos_idx = np.where(y_train_raw == 1)[0]\n",
    "    neg_idx = np.where(y_train_raw == 0)[0]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    neg_sample = rng.choice(neg_idx, len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    balanced_codes = [codes_train[i] for i in balanced_idx]\n",
    "    y_train = y_train_raw[balanced_idx]\n",
    "\n",
    "    X_train = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in balanced_codes)\n",
    "    X_val = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in codes_val)\n",
    "    X_test = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in codes_test)\n",
    "\n",
    "    X_train = np.expand_dims(np.stack(X_train).astype(np.float32) / 255.0, -1)\n",
    "    X_val = np.expand_dims(np.stack(X_val).astype(np.float32) / 255.0, -1)\n",
    "    X_test = np.expand_dims(np.stack(X_test).astype(np.float32) / 255.0, -1)\n",
    "\n",
    "    y_val = np.array(y_val).astype(np.int32)\n",
    "    y_test = np.array(y_test).astype(np.int32)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test), class_weight_dict\n",
    "\n",
    "def objective(trial):\n",
    "    try:\n",
    "        (X_train, X_val, _, y_train, y_val, _), class_weight = load_balanced_data(\"cwe119_cgd.txt\")\n",
    "    except ValueError as e:\n",
    "        raise optuna.exceptions.TrialPruned(str(e))\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-3, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    dense_units = trial.suggest_categorical(\"dense_units\", [64, 128, 256, 512])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(TARGET_SIZE, TARGET_SIZE, 1)),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Dropout(dropout),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation=\"relu\"),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=10, class_weight=class_weight, verbose=0)\n",
    "    y_val_probs = model.predict(X_val, batch_size=batch_size, verbose=0)\n",
    "    y_val_pred = np.argmax(y_val_probs, axis=1)\n",
    "\n",
    "    return np.mean(y_val_pred == y_val)  \n",
    "\n",
    "def train_best_model(best_params):\n",
    "    (X_trainval, _, X_test, y_trainval, _, y_test), class_weight = load_balanced_data(\"cwe119_cgd.txt\")\n",
    "\n",
    "    batch_size = best_params[\"batch_size\"]\n",
    "    trainval_ds = tf.data.Dataset.from_tensor_slices((X_trainval, y_trainval)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(TARGET_SIZE, TARGET_SIZE, 1)),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Dropout(best_params[\"dropout\"]),\n",
    "        Flatten(),\n",
    "        Dense(best_params[\"dense_units\"], activation=\"relu\"),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=best_params[\"learning_rate\"]),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining final model on train + val...\")\n",
    "    model.fit(trainval_ds, epochs=20, class_weight=class_weight, verbose=1)\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    y_pred_probs = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"\\nAccuracy        : {accuracy:.4f}\")\n",
    "    print(f\"True Pos Rate   : {tpr:.4f}\")\n",
    "    print(f\"False Neg Rate  : {fnr:.4f}\")\n",
    "    print(f\"False Pos Rate  : {fpr:.4f}\")\n",
    "    print(f\"Precision       : {precision:.4f}\")\n",
    "    print(f\"F1 Score        : {f1:.4f}\")\n",
    "\n",
    "def run_optuna():\n",
    "    compute_mean_coverage(\"cwe119_cgd.txt\", TARGET_SIZE)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(\"\\nBest Trial:\")\n",
    "    best = study.best_trial\n",
    "    print(f\"Accuracy: {best.value:.4f}\")\n",
    "    for k, v in best.params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    train_best_model(best.params)\n",
    "\n",
    "run_optuna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HYPERPARAMETER TUNING (CWE 399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-23 21:53:55,704] A new study created in memory with name: no-name-4298b01b-de8b-45a3-aa8e-59c084f261e3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Mean fraction of data preserved with TARGET_SIZE=32: 0.9353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-23 21:54:13,617] Trial 0 finished with value: 0.6368204659661946 and parameters: {'learning_rate': 4.970793410464982e-05, 'dropout': 0.35074186441874666, 'dense_units': 512, 'batch_size': 128}. Best is trial 0 with value: 0.6368204659661946.\n",
      "[I 2025-05-23 21:54:37,349] Trial 1 finished with value: 0.6391046139789859 and parameters: {'learning_rate': 2.0580541725067192e-05, 'dropout': 0.24182785242427024, 'dense_units': 64, 'batch_size': 32}. Best is trial 1 with value: 0.6391046139789859.\n",
      "[I 2025-05-23 21:54:53,651] Trial 2 finished with value: 0.8634079488350845 and parameters: {'learning_rate': 0.0028927308577403783, 'dropout': 0.47919020135162466, 'dense_units': 256, 'batch_size': 128}. Best is trial 2 with value: 0.8634079488350845.\n",
      "[I 2025-05-23 21:55:10,088] Trial 3 finished with value: 0.6057560529922339 and parameters: {'learning_rate': 1.9123017376983858e-05, 'dropout': 0.35086921361875245, 'dense_units': 128, 'batch_size': 128}. Best is trial 2 with value: 0.8634079488350845.\n",
      "[I 2025-05-23 21:55:31,691] Trial 4 finished with value: 0.7227044312471448 and parameters: {'learning_rate': 0.00010431103804858803, 'dropout': 0.44544952923981457, 'dense_units': 64, 'batch_size': 32}. Best is trial 2 with value: 0.8634079488350845.\n",
      "[I 2025-05-23 21:55:48,152] Trial 5 finished with value: 0.84376427592508 and parameters: {'learning_rate': 0.003581751156139765, 'dropout': 0.47392483339741226, 'dense_units': 128, 'batch_size': 128}. Best is trial 2 with value: 0.8634079488350845.\n",
      "[I 2025-05-23 21:56:06,137] Trial 6 finished with value: 0.8355413430790315 and parameters: {'learning_rate': 0.004845289217679507, 'dropout': 0.23621646104129676, 'dense_units': 128, 'batch_size': 128}. Best is trial 2 with value: 0.8634079488350845.\n",
      "[I 2025-05-23 21:56:25,754] Trial 7 finished with value: 0.8497030607583371 and parameters: {'learning_rate': 0.00297023745260014, 'dropout': 0.2490680491982379, 'dense_units': 512, 'batch_size': 64}. Best is trial 2 with value: 0.8634079488350845.\n",
      "[I 2025-05-23 21:56:43,024] Trial 8 finished with value: 0.6797624486066697 and parameters: {'learning_rate': 8.83693080548632e-05, 'dropout': 0.42330098877667277, 'dense_units': 512, 'batch_size': 128}. Best is trial 2 with value: 0.8634079488350845.\n",
      "[I 2025-05-23 21:56:59,393] Trial 9 finished with value: 0.8547281863864779 and parameters: {'learning_rate': 0.00044907483189209275, 'dropout': 0.23490069384307782, 'dense_units': 256, 'batch_size': 128}. Best is trial 2 with value: 0.8634079488350845.\n",
      "[I 2025-05-23 21:57:34,405] Trial 10 finished with value: 0.8652352672453175 and parameters: {'learning_rate': 0.0006785684785956022, 'dropout': 0.39939793046987, 'dense_units': 256, 'batch_size': 16}. Best is trial 10 with value: 0.8652352672453175.\n",
      "[I 2025-05-23 21:58:10,467] Trial 11 finished with value: 0.8602101416171768 and parameters: {'learning_rate': 0.0007119628634677987, 'dropout': 0.4972850263574333, 'dense_units': 256, 'batch_size': 16}. Best is trial 10 with value: 0.8652352672453175.\n",
      "[I 2025-05-23 21:58:44,229] Trial 12 finished with value: 0.8670625856555505 and parameters: {'learning_rate': 0.0011305988657157393, 'dropout': 0.404055842528977, 'dense_units': 256, 'batch_size': 16}. Best is trial 12 with value: 0.8670625856555505.\n",
      "[I 2025-05-23 21:59:17,499] Trial 13 finished with value: 0.8825947921425308 and parameters: {'learning_rate': 0.0006755173781308752, 'dropout': 0.39653164736085544, 'dense_units': 256, 'batch_size': 16}. Best is trial 13 with value: 0.8825947921425308.\n",
      "[I 2025-05-23 21:59:50,900] Trial 14 finished with value: 0.8679762448606669 and parameters: {'learning_rate': 0.0012859667931010214, 'dropout': 0.31076980251743813, 'dense_units': 256, 'batch_size': 16}. Best is trial 13 with value: 0.8825947921425308.\n",
      "[I 2025-05-23 22:00:24,594] Trial 15 finished with value: 0.8889904065783463 and parameters: {'learning_rate': 0.000292664961892896, 'dropout': 0.294643590864854, 'dense_units': 256, 'batch_size': 16}. Best is trial 15 with value: 0.8889904065783463.\n",
      "[I 2025-05-23 22:01:00,037] Trial 16 finished with value: 0.8487894015532207 and parameters: {'learning_rate': 0.00026268587337555604, 'dropout': 0.29584486680228306, 'dense_units': 256, 'batch_size': 16}. Best is trial 15 with value: 0.8889904065783463.\n",
      "[I 2025-05-23 22:01:18,140] Trial 17 finished with value: 0.8145271813613523 and parameters: {'learning_rate': 0.00022296554969196669, 'dropout': 0.29264369397896894, 'dense_units': 256, 'batch_size': 64}. Best is trial 15 with value: 0.8889904065783463.\n",
      "[I 2025-05-23 22:01:48,341] Trial 18 finished with value: 0.8268615806304248 and parameters: {'learning_rate': 0.00022499124113863859, 'dropout': 0.37953999795210225, 'dense_units': 64, 'batch_size': 16}. Best is trial 15 with value: 0.8889904065783463.\n",
      "[I 2025-05-23 22:02:22,542] Trial 19 finished with value: 0.8848789401553221 and parameters: {'learning_rate': 0.0014661060338209933, 'dropout': 0.31651907879445473, 'dense_units': 256, 'batch_size': 16}. Best is trial 15 with value: 0.8889904065783463.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Best Trial:\n",
      "Accuracy: 0.8890\n",
      "  learning_rate: 0.000292664961892896\n",
      "  dropout: 0.294643590864854\n",
      "  dense_units: 256\n",
      "  batch_size: 16\n",
      "\n",
      "Training final model on train + val...\n",
      "Epoch 1/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.5654 - loss: 0.6748\n",
      "Epoch 2/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6693 - loss: 0.5983\n",
      "Epoch 3/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7642 - loss: 0.4707\n",
      "Epoch 4/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8151 - loss: 0.3845\n",
      "Epoch 5/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8491 - loss: 0.3399\n",
      "Epoch 6/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8687 - loss: 0.2929\n",
      "Epoch 7/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8817 - loss: 0.2771\n",
      "Epoch 8/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.8906 - loss: 0.2496\n",
      "Epoch 9/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9031 - loss: 0.2251\n",
      "Epoch 10/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9124 - loss: 0.2074\n",
      "Epoch 11/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9231 - loss: 0.1878\n",
      "Epoch 12/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9283 - loss: 0.1765\n",
      "Epoch 13/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9333 - loss: 0.1586\n",
      "Epoch 14/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9430 - loss: 0.1428\n",
      "Epoch 15/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9472 - loss: 0.1336\n",
      "Epoch 16/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9512 - loss: 0.1207\n",
      "Epoch 17/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9595 - loss: 0.1046\n",
      "Epoch 18/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9619 - loss: 0.1027\n",
      "Epoch 19/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9621 - loss: 0.0931\n",
      "Epoch 20/20\n",
      "\u001b[1m638/638\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.9641 - loss: 0.0862\n",
      "\n",
      "Evaluating on test set...\n",
      "\n",
      "Accuracy        : 0.8919\n",
      "True Pos Rate   : 0.8840\n",
      "False Neg Rate  : 0.1160\n",
      "False Pos Rate  : 0.1041\n",
      "Precision       : 0.8090\n",
      "F1 Score        : 0.8449\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, f1_score\n",
    "from math import floor, sqrt\n",
    "import optuna\n",
    "\n",
    "SEED = 41\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "TARGET_SIZE = 32\n",
    "\n",
    "def compute_mean_coverage(filepath, target_size):\n",
    "    lengths = []\n",
    "    with open(filepath, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []\n",
    "        label = None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if gadget and label in [0, 1]:\n",
    "                    byte_array = bytearray(\"\\n\".join(gadget), 'utf-8')\n",
    "                    lengths.append(len(byte_array))\n",
    "                gadget = []\n",
    "                label = None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    label_candidate = int(stripped)\n",
    "                    if label_candidate in [0, 1]:\n",
    "                        label = label_candidate\n",
    "                    else:\n",
    "                        gadget = []\n",
    "                        label = None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "\n",
    "    max_pixels = target_size * target_size\n",
    "    coverage = [min(floor(sqrt(l))**2, max_pixels) / l for l in lengths if l > 0]\n",
    "    mean_coverage = np.mean(coverage)\n",
    "    print(f\"\\nMean fraction of data preserved with TARGET_SIZE={target_size}: {mean_coverage:.4f}\")\n",
    "    return mean_coverage\n",
    "\n",
    "def parse_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []\n",
    "        label = None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if gadget and label in [0, 1]:\n",
    "                    yield \"\\n\".join(gadget), label\n",
    "                gadget = []\n",
    "                label = None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    label_candidate = int(stripped)\n",
    "                    if label_candidate in [0, 1]:\n",
    "                        label = label_candidate\n",
    "                    else:\n",
    "                        gadget = []\n",
    "                        label = None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "\n",
    "def code_to_image(code_sample):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "    padded = np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    h = min(h, TARGET_SIZE)\n",
    "    w = min(w, TARGET_SIZE)\n",
    "    padded[:h, :w] = cropped[:h, :w]\n",
    "    return padded\n",
    "\n",
    "def load_balanced_data(filepath, seed=SEED):\n",
    "    codes, labels = zip(*parse_file(filepath))\n",
    "    labels = np.array(labels).astype(np.int32)\n",
    "\n",
    "    codes_trainval, codes_test, y_trainval_raw, y_test = train_test_split(\n",
    "        codes, labels, test_size=0.2, stratify=labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    codes_train, codes_val, y_train_raw, y_val = train_test_split(\n",
    "        codes_trainval, y_trainval_raw, test_size=0.125, stratify=y_trainval_raw, random_state=seed\n",
    "    )\n",
    "\n",
    "    y_train_raw = np.array(y_train_raw)\n",
    "    pos_idx = np.where(y_train_raw == 1)[0]\n",
    "    neg_idx = np.where(y_train_raw == 0)[0]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    neg_sample = rng.choice(neg_idx, len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    balanced_codes = [codes_train[i] for i in balanced_idx]\n",
    "    y_train = y_train_raw[balanced_idx]\n",
    "\n",
    "    X_train = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in balanced_codes)\n",
    "    X_val = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in codes_val)\n",
    "    X_test = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in codes_test)\n",
    "\n",
    "    X_train = np.expand_dims(np.stack(X_train).astype(np.float32) / 255.0, -1)\n",
    "    X_val = np.expand_dims(np.stack(X_val).astype(np.float32) / 255.0, -1)\n",
    "    X_test = np.expand_dims(np.stack(X_test).astype(np.float32) / 255.0, -1)\n",
    "\n",
    "    y_val = np.array(y_val).astype(np.int32)\n",
    "    y_test = np.array(y_test).astype(np.int32)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    return (X_train, X_val, X_test, y_train, y_val, y_test), class_weight_dict\n",
    "\n",
    "def objective(trial):\n",
    "    try:\n",
    "        (X_train, X_val, _, y_train, y_val, _), class_weight = load_balanced_data(\"cwe399_cgd.txt\")\n",
    "    except ValueError as e:\n",
    "        raise optuna.exceptions.TrialPruned(str(e))\n",
    "\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-3, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.2, 0.5)\n",
    "    dense_units = trial.suggest_categorical(\"dense_units\", [64, 128, 256, 512])\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [16, 32, 64, 128])\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    val_ds = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(TARGET_SIZE, TARGET_SIZE, 1)),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Dropout(dropout),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation=\"relu\"),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    model.fit(train_ds, validation_data=val_ds, epochs=10, class_weight=class_weight, verbose=0)\n",
    "    y_val_probs = model.predict(X_val, batch_size=batch_size, verbose=0)\n",
    "    y_val_pred = np.argmax(y_val_probs, axis=1)\n",
    "\n",
    "    return np.mean(y_val_pred == y_val)  \n",
    "\n",
    "def train_best_model(best_params):\n",
    "    (X_trainval, _, X_test, y_trainval, _, y_test), class_weight = load_balanced_data(\"cwe399_cgd.txt\")\n",
    "\n",
    "    batch_size = best_params[\"batch_size\"]\n",
    "    trainval_ds = tf.data.Dataset.from_tensor_slices((X_trainval, y_trainval)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(TARGET_SIZE, TARGET_SIZE, 1)),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Dropout(best_params[\"dropout\"]),\n",
    "        Flatten(),\n",
    "        Dense(best_params[\"dense_units\"], activation=\"relu\"),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=best_params[\"learning_rate\"]),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(\"\\nTraining final model on train + val...\")\n",
    "    model.fit(trainval_ds, epochs=20, class_weight=class_weight, verbose=1)\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    y_pred_probs = model.predict(X_test, batch_size=batch_size, verbose=0)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"\\nAccuracy        : {accuracy:.4f}\")\n",
    "    print(f\"True Pos Rate   : {tpr:.4f}\")\n",
    "    print(f\"False Neg Rate  : {fnr:.4f}\")\n",
    "    print(f\"False Pos Rate  : {fpr:.4f}\")\n",
    "    print(f\"Precision       : {precision:.4f}\")\n",
    "    print(f\"F1 Score        : {f1:.4f}\")\n",
    "\n",
    "def run_optuna():\n",
    "    compute_mean_coverage(\"cwe399_cgd.txt\", TARGET_SIZE)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(\"\\nBest Trial:\")\n",
    "    best = study.best_trial\n",
    "    print(f\"Accuracy: {best.value:.4f}\")\n",
    "    for k, v in best.params.items():\n",
    "        print(f\"  {k}: {v}\")\n",
    "\n",
    "    train_best_model(best.params)\n",
    "\n",
    "run_optuna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST MODEL (CWE 119)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.6018 - loss: 0.6519\n",
      "Epoch 2/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.6703 - loss: 0.5951\n",
      "Epoch 3/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7542 - loss: 0.4983\n",
      "Epoch 4/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 7ms/step - accuracy: 0.7828 - loss: 0.4491\n",
      "Epoch 5/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7962 - loss: 0.4218\n",
      "Epoch 6/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8133 - loss: 0.3909\n",
      "Epoch 7/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8217 - loss: 0.3702\n",
      "Epoch 8/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8342 - loss: 0.3539\n",
      "Epoch 9/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8398 - loss: 0.3394\n",
      "Epoch 10/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8512 - loss: 0.3266\n",
      "Epoch 11/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8544 - loss: 0.3090\n",
      "Epoch 12/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8602 - loss: 0.3029\n",
      "Epoch 13/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8665 - loss: 0.2889\n",
      "Epoch 14/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8760 - loss: 0.2714\n",
      "Epoch 15/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8821 - loss: 0.2634\n",
      "Epoch 16/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8869 - loss: 0.2559\n",
      "Epoch 17/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.8939 - loss: 0.2364\n",
      "Epoch 18/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9006 - loss: 0.2325\n",
      "Epoch 19/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.9022 - loss: 0.2242\n",
      "Epoch 20/20\n",
      "\u001b[1m1044/1044\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.9128 - loss: 0.2064\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m497/497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7975 - loss: 0.4989\n",
      "\n",
      "Final Test Accuracy: 0.8003\n",
      "\n",
      "Computing additional metrics...\n",
      "\u001b[1m497/497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step\n",
      "Accuracy        : 0.8003\n",
      "True Pos Rate   : 0.7840\n",
      "False Neg Rate  : 0.2160\n",
      "False Pos Rate  : 0.1939\n",
      "Precision       : 0.5901\n",
      "F1 Score        : 0.6734\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from math import floor, sqrt\n",
    "\n",
    "# ---------------------------------------\n",
    "# 0. Reproducibility Setup\n",
    "# ---------------------------------------\n",
    "SEED = 41\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Preprocessing \n",
    "# ---------------------------------------\n",
    "TARGET_SIZE = 32  \n",
    "\n",
    "def parse_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []\n",
    "        label = None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if gadget and label in [0, 1]:\n",
    "                    yield \"\\n\".join(gadget), label\n",
    "                gadget = []\n",
    "                label = None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    label_candidate = int(stripped)\n",
    "                    if label_candidate in [0, 1]:\n",
    "                        label = label_candidate\n",
    "                    else:\n",
    "                        gadget = []\n",
    "                        label = None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "\n",
    "def code_to_image(code_sample):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "\n",
    "    padded = np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    h = min(h, TARGET_SIZE)\n",
    "    w = min(w, TARGET_SIZE)\n",
    "    padded[:h, :w] = cropped[:h, :w]\n",
    "\n",
    "    return padded\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Data Loader with Fixed Leakage\n",
    "# ---------------------------------------\n",
    "def load_balanced_data(filepath, seed=SEED):\n",
    "    codes, labels = zip(*parse_file(filepath))\n",
    "    labels = np.array(labels).astype(np.int32)\n",
    "\n",
    "    # Step 1: Split first\n",
    "    codes_train, codes_test, y_train_raw, y_test = train_test_split(\n",
    "        codes, labels, test_size=0.2, stratify=labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Step 2: Balance training data only\n",
    "    y_train_raw = np.array(y_train_raw)\n",
    "    pos_idx = np.where(y_train_raw == 1)[0]\n",
    "    neg_idx = np.where(y_train_raw == 0)[0]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    neg_sample = rng.choice(neg_idx, len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    balanced_codes = [codes_train[i] for i in balanced_idx]\n",
    "    y_train = y_train_raw[balanced_idx]\n",
    "\n",
    "    # Step 3: Convert to image arrays\n",
    "    X_train = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in balanced_codes)\n",
    "    X_test = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in codes_test)\n",
    "\n",
    "    X_train = np.expand_dims(np.stack(X_train).astype(np.float32) / 255.0, -1)\n",
    "    X_test = np.expand_dims(np.stack(X_test).astype(np.float32) / 255.0, -1)\n",
    "    y_test = np.array(y_test).astype(np.int32)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test), class_weight_dict\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Training Function Using tf.data\n",
    "# ---------------------------------------\n",
    "def train_final_model(filepath):\n",
    "    (X_train, X_test, y_train, y_test), class_weight = load_balanced_data(filepath)\n",
    "\n",
    "    learning_rate = 0.00041314272442970695\n",
    "    dropout = 0.32317539094635517\n",
    "    dense_units = 512\n",
    "    batch_size = 16\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(TARGET_SIZE, TARGET_SIZE, 1)),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Dropout(dropout),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation=\"relu\"),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(\"Training for 20 epochs...\")\n",
    "    model.fit(train_ds, epochs=20, class_weight=class_weight, verbose=1)\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nComputing additional metrics...\")\n",
    "    y_pred_probs = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy        : {accuracy:.4f}\")\n",
    "    print(f\"True Pos Rate   : {tpr:.4f}\")\n",
    "    print(f\"False Neg Rate  : {fnr:.4f}\")\n",
    "    print(f\"False Pos Rate  : {fpr:.4f}\")\n",
    "    print(f\"Precision       : {precision:.4f}\")\n",
    "    print(f\"F1 Score        : {f1:.4f}\")\n",
    "\n",
    "# --- Run the model ---\n",
    "train_final_model(\"cwe119_cgd.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BEST MODEL (CWE 399)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.5648 - loss: 0.6747\n",
      "Epoch 2/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7012 - loss: 0.5513\n",
      "Epoch 3/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7876 - loss: 0.4264\n",
      "Epoch 4/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8244 - loss: 0.3664\n",
      "Epoch 5/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8497 - loss: 0.3288\n",
      "Epoch 6/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8673 - loss: 0.2928\n",
      "Epoch 7/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8755 - loss: 0.2699\n",
      "Epoch 8/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.8958 - loss: 0.2408\n",
      "Epoch 9/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9037 - loss: 0.2170\n",
      "Epoch 10/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9151 - loss: 0.2048\n",
      "Epoch 11/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9145 - loss: 0.1965\n",
      "Epoch 12/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9299 - loss: 0.1659\n",
      "Epoch 13/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9312 - loss: 0.1662\n",
      "Epoch 14/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9382 - loss: 0.1435\n",
      "Epoch 15/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9462 - loss: 0.1315\n",
      "Epoch 16/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9485 - loss: 0.1237\n",
      "Epoch 17/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.9523 - loss: 0.1136\n",
      "Epoch 18/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9504 - loss: 0.1237\n",
      "Epoch 19/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 7ms/step - accuracy: 0.9626 - loss: 0.0954\n",
      "Epoch 20/20\n",
      "\u001b[1m729/729\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 6ms/step - accuracy: 0.9636 - loss: 0.0940\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.9031 - loss: 0.2645\n",
      "\n",
      "Final Test Accuracy: 0.9008\n",
      "\n",
      "Computing additional metrics...\n",
      "\u001b[1m274/274\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Accuracy        : 0.9008\n",
      "True Pos Rate   : 0.8593\n",
      "False Neg Rate  : 0.1407\n",
      "False Pos Rate  : 0.0784\n",
      "Precision       : 0.8454\n",
      "F1 Score        : 0.8523\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from joblib import Parallel, delayed\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPool2D, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from math import floor, sqrt\n",
    "\n",
    "# ---------------------------------------\n",
    "# 0. Reproducibility Setup\n",
    "# ---------------------------------------\n",
    "SEED = 41\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(SEED)\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# ---------------------------------------\n",
    "# 1. Preprocessing \n",
    "# ---------------------------------------\n",
    "TARGET_SIZE = 32  \n",
    "\n",
    "def parse_file(filename):\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []\n",
    "        label = None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if gadget and label in [0, 1]:\n",
    "                    yield \"\\n\".join(gadget), label\n",
    "                gadget = []\n",
    "                label = None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    label_candidate = int(stripped)\n",
    "                    if label_candidate in [0, 1]:\n",
    "                        label = label_candidate\n",
    "                    else:\n",
    "                        gadget = []\n",
    "                        label = None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "\n",
    "def code_to_image(code_sample):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "\n",
    "    padded = np.zeros((TARGET_SIZE, TARGET_SIZE), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    h = min(h, TARGET_SIZE)\n",
    "    w = min(w, TARGET_SIZE)\n",
    "    padded[:h, :w] = cropped[:h, :w]\n",
    "\n",
    "    return padded\n",
    "\n",
    "# ---------------------------------------\n",
    "# 2. Data Loader with Fixed Leakage\n",
    "# ---------------------------------------\n",
    "def load_balanced_data(filepath, seed=SEED):\n",
    "    codes, labels = zip(*parse_file(filepath))\n",
    "    labels = np.array(labels).astype(np.int32)\n",
    "\n",
    "    # Step 1: Split first\n",
    "    codes_train, codes_test, y_train_raw, y_test = train_test_split(\n",
    "        codes, labels, test_size=0.2, stratify=labels, random_state=seed\n",
    "    )\n",
    "\n",
    "    # Step 2: Balance training data only\n",
    "    y_train_raw = np.array(y_train_raw)\n",
    "    pos_idx = np.where(y_train_raw == 1)[0]\n",
    "    neg_idx = np.where(y_train_raw == 0)[0]\n",
    "\n",
    "    rng = np.random.default_rng(seed)\n",
    "    neg_sample = rng.choice(neg_idx, len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    balanced_codes = [codes_train[i] for i in balanced_idx]\n",
    "    y_train = y_train_raw[balanced_idx]\n",
    "\n",
    "    # Step 3: Convert to image arrays\n",
    "    X_train = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in balanced_codes)\n",
    "    X_test = Parallel(n_jobs=-1)(delayed(code_to_image)(code) for code in codes_test)\n",
    "\n",
    "    X_train = np.expand_dims(np.stack(X_train).astype(np.float32) / 255.0, -1)\n",
    "    X_test = np.expand_dims(np.stack(X_test).astype(np.float32) / 255.0, -1)\n",
    "    y_test = np.array(y_test).astype(np.int32)\n",
    "\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=np.unique(y_train), y=y_train)\n",
    "    class_weight_dict = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "    return (X_train, X_test, y_train, y_test), class_weight_dict\n",
    "\n",
    "# ---------------------------------------\n",
    "# 3. Training Function Using tf.data\n",
    "# ---------------------------------------\n",
    "def train_final_model(filepath):\n",
    "    (X_train, X_test, y_train, y_test), class_weight = load_balanced_data(filepath)\n",
    "\n",
    "    learning_rate = 0.000292664961892896\n",
    "    dropout = 0.294643590864854\n",
    "    dense_units = 256\n",
    "    batch_size = 16\n",
    "\n",
    "    train_ds = tf.data.Dataset.from_tensor_slices((X_train, y_train)).shuffle(1000).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    test_ds = tf.data.Dataset.from_tensor_slices((X_test, y_test)).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "\n",
    "    model = Sequential([\n",
    "        tf.keras.Input(shape=(TARGET_SIZE, TARGET_SIZE, 1)),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(32, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Conv2D(64, 3, padding=\"same\", activation=\"relu\"),\n",
    "        MaxPool2D(),\n",
    "        Dropout(dropout),\n",
    "        Flatten(),\n",
    "        Dense(dense_units, activation=\"relu\"),\n",
    "        Dense(2, activation=\"softmax\")\n",
    "    ])\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=learning_rate),\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[\"accuracy\"]\n",
    "    )\n",
    "\n",
    "    print(\"Training for 20 epochs...\")\n",
    "    model.fit(train_ds, epochs=20, class_weight=class_weight, verbose=1)\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    loss, acc = model.evaluate(test_ds)\n",
    "    print(f\"\\nFinal Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    print(\"\\nComputing additional metrics...\")\n",
    "    y_pred_probs = model.predict(X_test, batch_size=batch_size)\n",
    "    y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    tpr = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    fnr = fn / (fn + tp) if (fn + tp) > 0 else 0\n",
    "    fpr = fp / (fp + tn) if (fp + tn) > 0 else 0\n",
    "    precision = precision_score(y_test, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_test, y_pred, zero_division=0)\n",
    "\n",
    "    print(f\"Accuracy        : {accuracy:.4f}\")\n",
    "    print(f\"True Pos Rate   : {tpr:.4f}\")\n",
    "    print(f\"False Neg Rate  : {fnr:.4f}\")\n",
    "    print(f\"False Pos Rate  : {fpr:.4f}\")\n",
    "    print(f\"Precision       : {precision:.4f}\")\n",
    "    print(f\"F1 Score        : {f1:.4f}\")\n",
    "\n",
    "# --- Run the model ---\n",
    "train_final_model(\"cwe399_cgd.txt\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
