{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58877b26",
   "metadata": {},
   "source": [
    "# CWE 399: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42d80a",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorized Code Gadget Input --> BiLSTM + Attention ------\\\n",
    "                                                            --> Concatenate --> Dense --> Output\n",
    "Vectorized Code Gadget Input --> CNN on expanded dims ----/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "751d7cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-24 17:45:50,533] A new study created in memory with name: no-name-ddde0659-c389-4be2-b3c9-9369051c9fab\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 14600, 1: 7285}\n",
      "Train split before balancing: {0: 11680, 1: 5828}\n",
      "Test set class distribution: {0: 2920, 1: 1457}\n",
      "Balanced training class distribution: {0: 5828, 1: 5828}\n",
      "Final training class distribution: {0: 4662, 1: 4662}\n",
      "Validation class distribution: {0: 1166, 1: 1166}\n",
      "Running Optuna hyperparameter tuning (with pruning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahusai\\AppData\\Local\\Temp\\ipykernel_6228\\2087809305.py:101: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2),\n",
      "[I 2025-05-24 17:48:43,549] Trial 0 finished with value: 0.9301029443740845 and parameters: {'lstm_units': 64, 'cnn_filters1': 16, 'cnn_filters2': 32, 'dense_units': 128, 'dropout_lstm': 0.43498018806936434, 'dropout_cnn': 0.29313006045819584, 'dropout_final': 0.33092831126041217, 'learning_rate': 0.0040012046538794276, 'batch_size': 32}. Best is trial 0 with value: 0.9301029443740845.\n",
      "[I 2025-05-24 17:53:16,403] Trial 1 finished with value: 0.9206689596176147 and parameters: {'lstm_units': 256, 'cnn_filters1': 64, 'cnn_filters2': 64, 'dense_units': 256, 'dropout_lstm': 0.21162349447376705, 'dropout_cnn': 0.4234054337603, 'dropout_final': 0.39768971433503764, 'learning_rate': 0.00019066966613780277, 'batch_size': 128}. Best is trial 0 with value: 0.9301029443740845.\n",
      "[I 2025-05-24 18:00:44,006] Trial 2 finished with value: 0.9335334300994873 and parameters: {'lstm_units': 256, 'cnn_filters1': 16, 'cnn_filters2': 128, 'dense_units': 128, 'dropout_lstm': 0.2593249474896073, 'dropout_cnn': 0.28207852342333034, 'dropout_final': 0.3878992341845834, 'learning_rate': 0.00031809362212212544, 'batch_size': 64}. Best is trial 2 with value: 0.9335334300994873.\n",
      "[I 2025-05-24 18:04:15,125] Trial 3 finished with value: 0.9176672101020813 and parameters: {'lstm_units': 128, 'cnn_filters1': 64, 'cnn_filters2': 64, 'dense_units': 256, 'dropout_lstm': 0.3115638988436854, 'dropout_cnn': 0.4362221698894477, 'dropout_final': 0.37448141621748926, 'learning_rate': 0.0024844143071648234, 'batch_size': 32}. Best is trial 2 with value: 0.9335334300994873.\n",
      "[I 2025-05-24 18:09:13,624] Trial 4 finished with value: 0.927101194858551 and parameters: {'lstm_units': 256, 'cnn_filters1': 64, 'cnn_filters2': 64, 'dense_units': 256, 'dropout_lstm': 0.4787293689868835, 'dropout_cnn': 0.38638760649528847, 'dropout_final': 0.2792995376831162, 'learning_rate': 0.00022135178946406032, 'batch_size': 128}. Best is trial 2 with value: 0.9335334300994873.\n",
      "[I 2025-05-24 18:12:47,956] Trial 5 finished with value: 0.9361063241958618 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 64, 'dense_units': 64, 'dropout_lstm': 0.4244323609572827, 'dropout_cnn': 0.33187450603056334, 'dropout_final': 0.3834739092296845, 'learning_rate': 0.0002574980986746612, 'batch_size': 32}. Best is trial 5 with value: 0.9361063241958618.\n",
      "[I 2025-05-24 18:14:50,850] Trial 6 finished with value: 0.9309605360031128 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 64, 'dense_units': 256, 'dropout_lstm': 0.24271164879674004, 'dropout_cnn': 0.347880351935533, 'dropout_final': 0.49520271003012656, 'learning_rate': 0.00024591313506691615, 'batch_size': 128}. Best is trial 5 with value: 0.9361063241958618.\n",
      "[I 2025-05-24 18:17:07,187] Trial 7 pruned. Trial was pruned at epoch 6.\n",
      "[I 2025-05-24 18:19:48,434] Trial 8 pruned. Trial was pruned at epoch 6.\n",
      "[I 2025-05-24 18:21:46,406] Trial 9 pruned. Trial was pruned at epoch 7.\n",
      "[I 2025-05-24 18:25:13,741] Trial 10 finished with value: 0.9373927712440491 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 32, 'dense_units': 64, 'dropout_lstm': 0.3684158314743855, 'dropout_cnn': 0.47970401354212167, 'dropout_final': 0.20944623329871623, 'learning_rate': 0.0007251925170101061, 'batch_size': 32}. Best is trial 10 with value: 0.9373927712440491.\n",
      "[I 2025-05-24 18:28:35,866] Trial 11 finished with value: 0.9301029443740845 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 32, 'dense_units': 64, 'dropout_lstm': 0.369993663732066, 'dropout_cnn': 0.47957118260902626, 'dropout_final': 0.22092916709363092, 'learning_rate': 0.0006668884050331709, 'batch_size': 32}. Best is trial 10 with value: 0.9373927712440491.\n",
      "[I 2025-05-24 18:31:55,742] Trial 12 finished with value: 0.9331046342849731 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 32, 'dense_units': 64, 'dropout_lstm': 0.3724950493024884, 'dropout_cnn': 0.32642960653988295, 'dropout_final': 0.20948820371339907, 'learning_rate': 0.000690033745074544, 'batch_size': 32}. Best is trial 10 with value: 0.9373927712440491.\n",
      "[I 2025-05-24 18:34:13,896] Trial 13 pruned. Trial was pruned at epoch 5.\n",
      "[I 2025-05-24 18:37:44,471] Trial 14 finished with value: 0.9356775283813477 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 128, 'dense_units': 64, 'dropout_lstm': 0.4716263123728717, 'dropout_cnn': 0.2119083583576617, 'dropout_final': 0.28588231912341106, 'learning_rate': 0.001048217547783558, 'batch_size': 32}. Best is trial 10 with value: 0.9373927712440491.\n",
      "[I 2025-05-24 18:41:23,722] Trial 15 finished with value: 0.9356775283813477 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 64, 'dense_units': 64, 'dropout_lstm': 0.34812569243263003, 'dropout_cnn': 0.32275430105772873, 'dropout_final': 0.46157010292845146, 'learning_rate': 0.000767426179120875, 'batch_size': 32}. Best is trial 10 with value: 0.9373927712440491.\n",
      "[I 2025-05-24 18:44:59,444] Trial 16 finished with value: 0.9403945207595825 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 32, 'dense_units': 64, 'dropout_lstm': 0.3354719785627817, 'dropout_cnn': 0.45088206221631577, 'dropout_final': 0.25222072883126656, 'learning_rate': 0.0004554889354724233, 'batch_size': 32}. Best is trial 16 with value: 0.9403945207595825.\n",
      "[I 2025-05-24 18:48:43,274] Trial 17 finished with value: 0.9352487325668335 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 32, 'dense_units': 64, 'dropout_lstm': 0.33553567371135024, 'dropout_cnn': 0.4529754166259527, 'dropout_final': 0.2335983503297721, 'learning_rate': 0.0004251749516801936, 'batch_size': 32}. Best is trial 16 with value: 0.9403945207595825.\n",
      "[I 2025-05-24 18:51:07,841] Trial 18 pruned. Trial was pruned at epoch 6.\n",
      "[I 2025-05-24 18:55:21,535] Trial 19 pruned. Trial was pruned at epoch 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: 16\n",
      "Best hyperparameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 32, 'dense_units': 64, 'dropout_lstm': 0.3354719785627817, 'dropout_cnn': 0.45088206221631577, 'dropout_final': 0.25222072883126656, 'learning_rate': 0.0004554889354724233, 'batch_size': 32}\n",
      "Epoch 1/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 71ms/step - accuracy: 0.6958 - loss: 0.5561 - learning_rate: 4.5549e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 66ms/step - accuracy: 0.8822 - loss: 0.2780 - learning_rate: 4.5549e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.9149 - loss: 0.2050 - learning_rate: 4.5549e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.9171 - loss: 0.1814 - learning_rate: 4.5549e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 68ms/step - accuracy: 0.9339 - loss: 0.1551 - learning_rate: 4.5549e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 65ms/step - accuracy: 0.9359 - loss: 0.1492 - learning_rate: 4.5549e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 66ms/step - accuracy: 0.9400 - loss: 0.1352 - learning_rate: 4.5549e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 65ms/step - accuracy: 0.9448 - loss: 0.1250 - learning_rate: 4.5549e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 67ms/step - accuracy: 0.9483 - loss: 0.1206 - learning_rate: 4.5549e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 66ms/step - accuracy: 0.9495 - loss: 0.1138 - learning_rate: 4.5549e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 70ms/step - accuracy: 0.9515 - loss: 0.1112 - learning_rate: 4.5549e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 66ms/step - accuracy: 0.9471 - loss: 0.1123 - learning_rate: 4.5549e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 66ms/step - accuracy: 0.9508 - loss: 0.1128 - learning_rate: 4.5549e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 65ms/step - accuracy: 0.9511 - loss: 0.1061 - learning_rate: 4.5549e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.9525 - loss: 0.1069 - learning_rate: 4.5549e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.9529 - loss: 0.1014 - learning_rate: 4.5549e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.9506 - loss: 0.1023 - learning_rate: 4.5549e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.9510 - loss: 0.1022 - learning_rate: 4.5549e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.9529 - loss: 0.0991 - learning_rate: 4.5549e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 63ms/step - accuracy: 0.9533 - loss: 0.0971 - learning_rate: 4.5549e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 63ms/step - accuracy: 0.9540 - loss: 0.0960 - learning_rate: 4.5549e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 63ms/step - accuracy: 0.9553 - loss: 0.0920 - learning_rate: 4.5549e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 63ms/step - accuracy: 0.9558 - loss: 0.0917 - learning_rate: 4.5549e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 63ms/step - accuracy: 0.9546 - loss: 0.0924 - learning_rate: 4.5549e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.9563 - loss: 0.0912 - learning_rate: 4.5549e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 66ms/step - accuracy: 0.9574 - loss: 0.0906 - learning_rate: 4.5549e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 65ms/step - accuracy: 0.9537 - loss: 0.0933 - learning_rate: 4.5549e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 62ms/step - accuracy: 0.9570 - loss: 0.0898 - learning_rate: 4.5549e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.9571 - loss: 0.0876 - learning_rate: 4.5549e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 64ms/step - accuracy: 0.9563 - loss: 0.0897 - learning_rate: 4.5549e-04\n",
      "Restoring model weights from the end of the best epoch: 29.\n",
      "\n",
      "Evaluating model on test set...\n",
      "Test Accuracy: 0.9326\n",
      "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 29ms/step\n",
      "TPR (Recall): 0.9348\n",
      "FPR         : 0.0685\n",
      "FNR         : 0.0652\n",
      "Precision   : 0.8720\n",
      "F1 Score    : 0.9023\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, LeakyReLU,\n",
    "    LSTM, Bidirectional, Concatenate, Lambda\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 41\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# -----------------------------\n",
    "# Load vector data\n",
    "# -----------------------------\n",
    "def load_vector_data(filename):\n",
    "    df = pd.read_pickle(filename)\n",
    "    vectors = np.stack(df[\"vector\"].values).astype(np.float32)\n",
    "    labels = df[\"val\"].values.astype(int)\n",
    "    return vectors, labels\n",
    "\n",
    "# -----------------------------\n",
    "# Custom attention layer\n",
    "# -----------------------------\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "# -----------------------------\n",
    "# Build model with hyperparams\n",
    "# -----------------------------\n",
    "def build_model(input_shape, hp):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x_lstm = Bidirectional(LSTM(hp[\"lstm_units\"], return_sequences=True))(input_layer)\n",
    "    x_lstm = AttentionLayer()(x_lstm)\n",
    "    x_lstm = Dense(hp[\"dense_units\"])(x_lstm)\n",
    "    x_lstm = LeakyReLU()(x_lstm)\n",
    "    x_lstm = Dropout(hp[\"dropout_lstm\"])(x_lstm)\n",
    "\n",
    "    x_cnn = Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_layer)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    x_cnn = Dense(hp[\"dense_units\"], activation=\"relu\")(x_cnn)\n",
    "    x_cnn = Dropout(hp[\"dropout_cnn\"])(x_cnn)\n",
    "\n",
    "    merged = Concatenate()([x_lstm, x_cnn])\n",
    "    merged = Dense(hp[\"dense_units\"], activation=\"relu\")(merged)\n",
    "    merged = Dropout(hp[\"dropout_final\"])(merged)\n",
    "    output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(hp[\"learning_rate\"]), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Objective function for Optuna (with pruning)\n",
    "# -----------------------------\n",
    "def objective(trial):\n",
    "    hp = {\n",
    "        \"lstm_units\": trial.suggest_categorical(\"lstm_units\", [64, 128, 256]),\n",
    "        \"cnn_filters1\": trial.suggest_categorical(\"cnn_filters1\", [16, 32, 64]),\n",
    "        \"cnn_filters2\": trial.suggest_categorical(\"cnn_filters2\", [32, 64, 128]),\n",
    "        \"dense_units\": trial.suggest_categorical(\"dense_units\", [64, 128, 256]),\n",
    "        \"dropout_lstm\": trial.suggest_float(\"dropout_lstm\", 0.2, 0.5),\n",
    "        \"dropout_cnn\": trial.suggest_float(\"dropout_cnn\", 0.2, 0.5),\n",
    "        \"dropout_final\": trial.suggest_float(\"dropout_final\", 0.2, 0.5),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "    }\n",
    "\n",
    "    model = build_model(input_shape=X_train.shape[1:], hp=hp)\n",
    "\n",
    "    early_stopper = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=hp[\"batch_size\"],\n",
    "        callbacks=[\n",
    "            early_stopper,\n",
    "            TFKerasPruningCallback(trial, \"val_accuracy\")  # ðŸ’¡ pruning callback\n",
    "        ],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    val_accuracy = max(history.history[\"val_accuracy\"])\n",
    "    return val_accuracy\n",
    "\n",
    "# -----------------------------\n",
    "# Train function w/ tuning\n",
    "# -----------------------------\n",
    "def train_hybrid_cnn_lstm(pkl_file):\n",
    "    global X_train, X_val, y_train, y_val\n",
    "\n",
    "    X, y = load_vector_data(pkl_file)\n",
    "    print(f\"Original dataset class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "\n",
    "    X_train_all, X_test, y_train_all, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Train split before balancing: {dict(zip(*np.unique(y_train_all, return_counts=True)))}\")\n",
    "    print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    # Balance training set\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    X_train_balanced = X_train_all[balanced_idx]\n",
    "    y_train_balanced = y_train_all[balanced_idx]\n",
    "    print(f\"Balanced training class distribution: {dict(zip(*np.unique(y_train_balanced, return_counts=True)))}\")\n",
    "\n",
    "    # Validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_balanced, y_train_balanced, test_size=0.2, stratify=y_train_balanced, random_state=SEED\n",
    "    )\n",
    "    print(f\"Final training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "    print(f\"Validation class distribution: {dict(zip(*np.unique(y_val, return_counts=True)))}\")\n",
    "\n",
    "    # Run Optuna with pruning enabled\n",
    "    print(\"Running Optuna hyperparameter tuning (with pruning)...\")\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best hyperparameters: {study.best_params}\")\n",
    "\n",
    "    # Retrain best model on full training (train + val)\n",
    "    best_hp = study.best_params\n",
    "    X_full = np.concatenate([X_train, X_val])\n",
    "    y_full = np.concatenate([y_train, y_val])\n",
    "    model = build_model(input_shape=X_full.shape[1:], hp=best_hp)\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=3, verbose=1)\n",
    "    early_stopper = EarlyStopping(monitor=\"loss\", patience=2, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        X_full, y_full,\n",
    "        epochs=30,\n",
    "        batch_size=best_hp[\"batch_size\"],\n",
    "        callbacks=[lr_scheduler, early_stopper],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "    print(f\"FPR         : {fpr:.4f}\")\n",
    "    print(f\"FNR         : {fnr:.4f}\")\n",
    "    print(f\"Precision   : {precision:.4f}\")\n",
    "    print(f\"F1 Score    : {f1:.4f}\")\n",
    "\n",
    "# Run\n",
    "train_hybrid_cnn_lstm(\"cwe399_cgd_gadget_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f28a91",
   "metadata": {},
   "source": [
    "# CWE 399: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d899b852",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorized Code Gadget Input --> BiLSTM + Attention ------\\\n",
    "                                                            --> Concatenate --> Dense --> Output\n",
    "Vectorized Code Gadget Input --> CNN on expanded dims ----/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4e94a75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 14600, 1: 7285}\n",
      "Train split before balancing: {0: 11680, 1: 5828}\n",
      "Test set class distribution: {0: 2920, 1: 1457}\n",
      "Balanced training class distribution: {0: 5828, 1: 5828}\n",
      "Epoch 1/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 76ms/step - accuracy: 0.7027 - loss: 0.5382 - learning_rate: 4.5549e-04\n",
      "Epoch 2/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 75ms/step - accuracy: 0.8807 - loss: 0.2822 - learning_rate: 4.5549e-04\n",
      "Epoch 3/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 75ms/step - accuracy: 0.9063 - loss: 0.2192 - learning_rate: 4.5549e-04\n",
      "Epoch 4/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9165 - loss: 0.1910 - learning_rate: 4.5549e-04\n",
      "Epoch 5/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9241 - loss: 0.1720 - learning_rate: 4.5549e-04\n",
      "Epoch 6/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9296 - loss: 0.1586 - learning_rate: 4.5549e-04\n",
      "Epoch 7/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9386 - loss: 0.1411 - learning_rate: 4.5549e-04\n",
      "Epoch 8/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 79ms/step - accuracy: 0.9428 - loss: 0.1320 - learning_rate: 4.5549e-04\n",
      "Epoch 9/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 74ms/step - accuracy: 0.9466 - loss: 0.1229 - learning_rate: 4.5549e-04\n",
      "Epoch 10/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 73ms/step - accuracy: 0.9459 - loss: 0.1211 - learning_rate: 4.5549e-04\n",
      "Epoch 11/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.9499 - loss: 0.1110 - learning_rate: 4.5549e-04\n",
      "Epoch 12/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.9529 - loss: 0.1037 - learning_rate: 4.5549e-04\n",
      "Epoch 13/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m26s\u001b[0m 72ms/step - accuracy: 0.9506 - loss: 0.1021 - learning_rate: 4.5549e-04\n",
      "Epoch 14/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 73ms/step - accuracy: 0.9543 - loss: 0.0997 - learning_rate: 4.5549e-04\n",
      "Epoch 15/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9515 - loss: 0.1034 - learning_rate: 4.5549e-04\n",
      "Epoch 16/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9546 - loss: 0.0915 - learning_rate: 4.5549e-04\n",
      "Epoch 17/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 77ms/step - accuracy: 0.9553 - loss: 0.0938 - learning_rate: 4.5549e-04\n",
      "Epoch 18/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9568 - loss: 0.0931 - learning_rate: 4.5549e-04\n",
      "Epoch 19/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9568 - loss: 0.0885 - learning_rate: 4.5549e-04\n",
      "Epoch 20/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 79ms/step - accuracy: 0.9568 - loss: 0.0869 - learning_rate: 4.5549e-04\n",
      "Epoch 21/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 80ms/step - accuracy: 0.9571 - loss: 0.0922 - learning_rate: 4.5549e-04\n",
      "Epoch 22/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 84ms/step - accuracy: 0.9601 - loss: 0.0852 - learning_rate: 4.5549e-04\n",
      "Epoch 23/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 87ms/step - accuracy: 0.9594 - loss: 0.0831 - learning_rate: 4.5549e-04\n",
      "Epoch 24/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9579 - loss: 0.0879 - learning_rate: 4.5549e-04\n",
      "Epoch 25/30\n",
      "\u001b[1m364/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 76ms/step - accuracy: 0.9631 - loss: 0.0813\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 0.00022774447279516608.\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 76ms/step - accuracy: 0.9631 - loss: 0.0813 - learning_rate: 4.5549e-04\n",
      "Epoch 26/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 79ms/step - accuracy: 0.9608 - loss: 0.0821 - learning_rate: 2.2774e-04\n",
      "Epoch 27/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 85ms/step - accuracy: 0.9622 - loss: 0.0806 - learning_rate: 2.2774e-04\n",
      "Epoch 28/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 84ms/step - accuracy: 0.9622 - loss: 0.0784 - learning_rate: 2.2774e-04\n",
      "Epoch 29/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 84ms/step - accuracy: 0.9579 - loss: 0.0794 - learning_rate: 2.2774e-04\n",
      "Epoch 30/30\n",
      "\u001b[1m365/365\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 78ms/step - accuracy: 0.9618 - loss: 0.0767 - learning_rate: 2.2774e-04\n",
      "\n",
      "Evaluating model on test set...\n",
      "Test Accuracy: 0.9285\n",
      "\u001b[1m137/137\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 35ms/step\n",
      "TPR (Recall): 0.9554\n",
      "FPR         : 0.0849\n",
      "FNR         : 0.0446\n",
      "Precision   : 0.8488\n",
      "F1 Score    : 0.8989\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, LeakyReLU,\n",
    "    LSTM, Bidirectional, Concatenate, Lambda\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 41\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# -----------------------------\n",
    "# Load vector data\n",
    "# -----------------------------\n",
    "def load_vector_data(filename):\n",
    "    df = pd.read_pickle(filename)\n",
    "    vectors = np.stack(df[\"vector\"].values).astype(np.float32)\n",
    "    labels = df[\"val\"].values.astype(int)\n",
    "    return vectors, labels\n",
    "\n",
    "# -----------------------------\n",
    "# Custom attention layer\n",
    "# -----------------------------\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "# -----------------------------\n",
    "# Build model with best hyperparams\n",
    "# -----------------------------\n",
    "def build_best_model(input_shape):\n",
    "    hp = {\n",
    "        'lstm_units': 64,\n",
    "        'cnn_filters1': 32,\n",
    "        'cnn_filters2': 32,\n",
    "        'dense_units': 64,\n",
    "        'dropout_lstm': 0.3354719785627817,\n",
    "        'dropout_cnn': 0.45088206221631577,\n",
    "        'dropout_final': 0.25222072883126656,\n",
    "        'learning_rate': 0.0004554889354724233\n",
    "    }\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x_lstm = Bidirectional(LSTM(hp[\"lstm_units\"], return_sequences=True))(input_layer)\n",
    "    x_lstm = AttentionLayer()(x_lstm)\n",
    "    x_lstm = Dense(hp[\"dense_units\"])(x_lstm)\n",
    "    x_lstm = LeakyReLU()(x_lstm)\n",
    "    x_lstm = Dropout(hp[\"dropout_lstm\"])(x_lstm)\n",
    "\n",
    "    x_cnn = Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_layer)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    x_cnn = Dense(hp[\"dense_units\"], activation=\"relu\")(x_cnn)\n",
    "    x_cnn = Dropout(hp[\"dropout_cnn\"])(x_cnn)\n",
    "\n",
    "    merged = Concatenate()([x_lstm, x_cnn])\n",
    "    merged = Dense(hp[\"dense_units\"], activation=\"relu\")(merged)\n",
    "    merged = Dropout(hp[\"dropout_final\"])(merged)\n",
    "    output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(hp[\"learning_rate\"]), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Training and evaluation\n",
    "# -----------------------------\n",
    "def train_final_model(pkl_file):\n",
    "    X, y = load_vector_data(pkl_file)\n",
    "    print(f\"Original dataset class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "\n",
    "    X_train_all, X_test, y_train_all, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "    )\n",
    "    print(f\"Train split before balancing: {dict(zip(*np.unique(y_train_all, return_counts=True)))}\")\n",
    "    print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    # Balance training set (undersample class 0)\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    X_train = X_train_all[balanced_idx]\n",
    "    y_train = y_train_all[balanced_idx]\n",
    "    print(f\"Balanced training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "    model = build_best_model(input_shape=X_train.shape[1:])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        callbacks=[lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "    print(f\"FPR         : {fpr:.4f}\")\n",
    "    print(f\"FNR         : {fnr:.4f}\")\n",
    "    print(f\"Precision   : {precision:.4f}\")\n",
    "    print(f\"F1 Score    : {f1:.4f}\")\n",
    "\n",
    "# Run\n",
    "train_final_model(\"cwe399_cgd_gadget_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a373516a",
   "metadata": {},
   "source": [
    "# CWE 119: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b84ccd",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorized Code Gadget Input --> BiLSTM + Attention ------\\\n",
    "                                                            --> Concatenate --> Dense --> Output\n",
    "Vectorized Code Gadget Input --> CNN on expanded dims ----/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2c05a249",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 29313, 1: 10440}\n",
      "Train split before balancing: {0: 23450, 1: 8352}\n",
      "Test set class distribution: {0: 5863, 1: 2088}\n",
      "Balanced training class distribution: {0: 8352, 1: 8352}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-24 20:37:11,526] A new study created in memory with name: no-name-13090652-d3d3-4dd0-b881-b108806cc3cc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final training class distribution: {0: 6681, 1: 6682}\n",
      "Validation class distribution: {0: 1671, 1: 1670}\n",
      "Running Optuna hyperparameter tuning (with pruning)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Mahusai\\AppData\\Local\\Temp\\ipykernel_6228\\2445183410.py:101: FutureWarning: suggest_loguniform has been deprecated in v3.0.0. This feature will be removed in v6.0.0. See https://github.com/optuna/optuna/releases/tag/v3.0.0. Use suggest_float(..., log=True) instead.\n",
      "  \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2),\n",
      "[I 2025-05-24 20:44:27,047] Trial 0 finished with value: 0.8258006572723389 and parameters: {'lstm_units': 256, 'cnn_filters1': 64, 'cnn_filters2': 64, 'dense_units': 64, 'dropout_lstm': 0.36577380531075965, 'dropout_cnn': 0.24058217131266618, 'dropout_final': 0.22533772040065905, 'learning_rate': 0.0009890403095221465, 'batch_size': 128}. Best is trial 0 with value: 0.8258006572723389.\n",
      "[I 2025-05-24 20:48:05,837] Trial 1 finished with value: 0.838371753692627 and parameters: {'lstm_units': 64, 'cnn_filters1': 16, 'cnn_filters2': 64, 'dense_units': 128, 'dropout_lstm': 0.20612973550888686, 'dropout_cnn': 0.32762572826322267, 'dropout_final': 0.3392077049654727, 'learning_rate': 0.002956332636611597, 'batch_size': 128}. Best is trial 1 with value: 0.838371753692627.\n",
      "[I 2025-05-24 20:56:08,216] Trial 2 finished with value: 0.8150254487991333 and parameters: {'lstm_units': 256, 'cnn_filters1': 64, 'cnn_filters2': 128, 'dense_units': 64, 'dropout_lstm': 0.21016196195647574, 'dropout_cnn': 0.3753163869090642, 'dropout_final': 0.4292141709903565, 'learning_rate': 0.0003358653651804528, 'batch_size': 128}. Best is trial 1 with value: 0.838371753692627.\n",
      "[I 2025-05-24 20:59:45,193] Trial 3 finished with value: 0.8314875960350037 and parameters: {'lstm_units': 128, 'cnn_filters1': 16, 'cnn_filters2': 64, 'dense_units': 64, 'dropout_lstm': 0.24916640506843907, 'dropout_cnn': 0.30958926391135666, 'dropout_final': 0.4264905872893991, 'learning_rate': 0.008914649760585001, 'batch_size': 128}. Best is trial 1 with value: 0.838371753692627.\n",
      "[I 2025-05-24 21:04:07,879] Trial 4 finished with value: 0.8165220022201538 and parameters: {'lstm_units': 128, 'cnn_filters1': 32, 'cnn_filters2': 128, 'dense_units': 128, 'dropout_lstm': 0.44900633880074275, 'dropout_cnn': 0.2527285640078908, 'dropout_final': 0.30923388828014636, 'learning_rate': 0.006356702770332105, 'batch_size': 128}. Best is trial 1 with value: 0.838371753692627.\n",
      "[I 2025-05-24 21:07:47,186] Trial 5 finished with value: 0.841065526008606 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 128, 'dense_units': 64, 'dropout_lstm': 0.23250103597802382, 'dropout_cnn': 0.3826710293852184, 'dropout_final': 0.2623207469071867, 'learning_rate': 0.0040538954478365995, 'batch_size': 64}. Best is trial 5 with value: 0.841065526008606.\n",
      "[I 2025-05-24 21:16:54,064] Trial 6 pruned. Trial was pruned at epoch 5.\n",
      "[I 2025-05-24 21:22:51,334] Trial 7 finished with value: 0.8431607484817505 and parameters: {'lstm_units': 128, 'cnn_filters1': 16, 'cnn_filters2': 32, 'dense_units': 128, 'dropout_lstm': 0.3946052251938623, 'dropout_cnn': 0.3900848497439904, 'dropout_final': 0.21345658079480226, 'learning_rate': 0.0012561055586646933, 'batch_size': 32}. Best is trial 7 with value: 0.8431607484817505.\n",
      "[I 2025-05-24 21:25:35,857] Trial 8 pruned. Trial was pruned at epoch 5.\n",
      "[I 2025-05-24 21:27:40,922] Trial 9 pruned. Trial was pruned at epoch 5.\n",
      "[I 2025-05-24 21:31:26,934] Trial 10 pruned. Trial was pruned at epoch 5.\n",
      "[I 2025-05-24 21:35:22,047] Trial 11 finished with value: 0.8353786468505859 and parameters: {'lstm_units': 64, 'cnn_filters1': 32, 'cnn_filters2': 128, 'dense_units': 64, 'dropout_lstm': 0.306200801920321, 'dropout_cnn': 0.39734212350560794, 'dropout_final': 0.21568001315382848, 'learning_rate': 0.0019489621141998486, 'batch_size': 64}. Best is trial 7 with value: 0.8431607484817505.\n",
      "[I 2025-05-24 21:43:43,465] Trial 12 finished with value: 0.8371744751930237 and parameters: {'lstm_units': 128, 'cnn_filters1': 32, 'cnn_filters2': 128, 'dense_units': 128, 'dropout_lstm': 0.2864283293050274, 'dropout_cnn': 0.49458196733158544, 'dropout_final': 0.26560272657018724, 'learning_rate': 0.0033646960521695464, 'batch_size': 64}. Best is trial 7 with value: 0.8431607484817505.\n",
      "[I 2025-05-24 21:50:21,396] Trial 13 finished with value: 0.8389703631401062 and parameters: {'lstm_units': 64, 'cnn_filters1': 16, 'cnn_filters2': 32, 'dense_units': 64, 'dropout_lstm': 0.403390593789487, 'dropout_cnn': 0.4012486322603551, 'dropout_final': 0.2687964170304276, 'learning_rate': 0.0005986492562751671, 'batch_size': 32}. Best is trial 7 with value: 0.8431607484817505.\n",
      "[I 2025-05-24 21:55:29,794] Trial 14 pruned. Trial was pruned at epoch 5.\n",
      "[I 2025-05-24 22:01:12,221] Trial 15 finished with value: 0.8377731442451477 and parameters: {'lstm_units': 128, 'cnn_filters1': 16, 'cnn_filters2': 32, 'dense_units': 128, 'dropout_lstm': 0.3411575996630298, 'dropout_cnn': 0.37604942510750405, 'dropout_final': 0.3672816724032351, 'learning_rate': 0.0014837383517248592, 'batch_size': 32}. Best is trial 7 with value: 0.8431607484817505.\n",
      "[I 2025-05-24 22:03:18,559] Trial 16 pruned. Trial was pruned at epoch 5.\n",
      "[I 2025-05-24 22:09:56,260] Trial 17 finished with value: 0.8407662510871887 and parameters: {'lstm_units': 128, 'cnn_filters1': 16, 'cnn_filters2': 128, 'dense_units': 64, 'dropout_lstm': 0.4993680373897893, 'dropout_cnn': 0.2738742758462056, 'dropout_final': 0.29576176673719773, 'learning_rate': 0.00044853566487009167, 'batch_size': 32}. Best is trial 7 with value: 0.8431607484817505.\n",
      "[I 2025-05-24 22:12:04,425] Trial 18 pruned. Trial was pruned at epoch 5.\n",
      "[I 2025-05-24 22:22:50,964] Trial 19 pruned. Trial was pruned at epoch 5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: 7\n",
      "Best hyperparameters: {'lstm_units': 128, 'cnn_filters1': 16, 'cnn_filters2': 32, 'dense_units': 128, 'dropout_lstm': 0.3946052251938623, 'dropout_cnn': 0.3900848497439904, 'dropout_final': 0.21345658079480226, 'learning_rate': 0.0012561055586646933, 'batch_size': 32}\n",
      "Epoch 1/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 78ms/step - accuracy: 0.6571 - loss: 0.6038 - learning_rate: 0.0013\n",
      "Epoch 2/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.7855 - loss: 0.4582 - learning_rate: 0.0013\n",
      "Epoch 3/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.8218 - loss: 0.3875 - learning_rate: 0.0013\n",
      "Epoch 4/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.8474 - loss: 0.3419 - learning_rate: 0.0013\n",
      "Epoch 5/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.8630 - loss: 0.3076 - learning_rate: 0.0013\n",
      "Epoch 6/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.8757 - loss: 0.2807 - learning_rate: 0.0013\n",
      "Epoch 7/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.8839 - loss: 0.2626 - learning_rate: 0.0013\n",
      "Epoch 8/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.8840 - loss: 0.2545 - learning_rate: 0.0013\n",
      "Epoch 9/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.8937 - loss: 0.2408 - learning_rate: 0.0013\n",
      "Epoch 10/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.8990 - loss: 0.2295 - learning_rate: 0.0013\n",
      "Epoch 11/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9040 - loss: 0.2169 - learning_rate: 0.0013\n",
      "Epoch 12/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.9034 - loss: 0.2168 - learning_rate: 0.0013\n",
      "Epoch 13/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.9088 - loss: 0.2056 - learning_rate: 0.0013\n",
      "Epoch 14/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9075 - loss: 0.2039 - learning_rate: 0.0013\n",
      "Epoch 15/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9107 - loss: 0.1965 - learning_rate: 0.0013\n",
      "Epoch 16/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9143 - loss: 0.1885 - learning_rate: 0.0013\n",
      "Epoch 17/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9120 - loss: 0.1962 - learning_rate: 0.0013\n",
      "Epoch 18/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9157 - loss: 0.1886 - learning_rate: 0.0013\n",
      "Epoch 19/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9189 - loss: 0.1807 - learning_rate: 0.0013\n",
      "Epoch 20/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9221 - loss: 0.1716 - learning_rate: 0.0013\n",
      "Epoch 21/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9220 - loss: 0.1774 - learning_rate: 0.0013\n",
      "Epoch 22/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 78ms/step - accuracy: 0.9232 - loss: 0.1750 - learning_rate: 0.0013\n",
      "Epoch 23/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.9262 - loss: 0.1676 - learning_rate: 0.0013\n",
      "Epoch 24/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.9227 - loss: 0.1695 - learning_rate: 0.0013\n",
      "Epoch 25/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 79ms/step - accuracy: 0.9237 - loss: 0.1709 - learning_rate: 0.0013\n",
      "Epoch 26/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 95ms/step - accuracy: 0.9261 - loss: 0.1671 - learning_rate: 0.0013\n",
      "Epoch 27/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 92ms/step - accuracy: 0.9255 - loss: 0.1635 - learning_rate: 0.0013\n",
      "Epoch 28/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 89ms/step - accuracy: 0.9261 - loss: 0.1675 - learning_rate: 0.0013\n",
      "Epoch 29/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 87ms/step - accuracy: 0.9307 - loss: 0.1626 - learning_rate: 0.0013\n",
      "Epoch 29: early stopping\n",
      "Restoring model weights from the end of the best epoch: 27.\n",
      "\n",
      "Evaluating model on test set...\n",
      "Test Accuracy: 0.8321\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 49ms/step\n",
      "TPR (Recall): 0.8381\n",
      "FPR         : 0.1700\n",
      "FNR         : 0.1619\n",
      "Precision   : 0.6371\n",
      "F1 Score    : 0.7239\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, LeakyReLU,\n",
    "    LSTM, Bidirectional, Concatenate, Lambda\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 41\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# -----------------------------\n",
    "# Load vector data\n",
    "# -----------------------------\n",
    "def load_vector_data(filename):\n",
    "    df = pd.read_pickle(filename)\n",
    "    vectors = np.stack(df[\"vector\"].values).astype(np.float32)\n",
    "    labels = df[\"val\"].values.astype(int)\n",
    "    return vectors, labels\n",
    "\n",
    "# -----------------------------\n",
    "# Custom attention layer\n",
    "# -----------------------------\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "# -----------------------------\n",
    "# Build model with hyperparams\n",
    "# -----------------------------\n",
    "def build_model(input_shape, hp):\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x_lstm = Bidirectional(LSTM(hp[\"lstm_units\"], return_sequences=True))(input_layer)\n",
    "    x_lstm = AttentionLayer()(x_lstm)\n",
    "    x_lstm = Dense(hp[\"dense_units\"])(x_lstm)\n",
    "    x_lstm = LeakyReLU()(x_lstm)\n",
    "    x_lstm = Dropout(hp[\"dropout_lstm\"])(x_lstm)\n",
    "\n",
    "    x_cnn = Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_layer)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    x_cnn = Dense(hp[\"dense_units\"], activation=\"relu\")(x_cnn)\n",
    "    x_cnn = Dropout(hp[\"dropout_cnn\"])(x_cnn)\n",
    "\n",
    "    merged = Concatenate()([x_lstm, x_cnn])\n",
    "    merged = Dense(hp[\"dense_units\"], activation=\"relu\")(merged)\n",
    "    merged = Dropout(hp[\"dropout_final\"])(merged)\n",
    "    output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(hp[\"learning_rate\"]), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Objective function for Optuna (with pruning)\n",
    "# -----------------------------\n",
    "def objective(trial):\n",
    "    hp = {\n",
    "        \"lstm_units\": trial.suggest_categorical(\"lstm_units\", [64, 128, 256]),\n",
    "        \"cnn_filters1\": trial.suggest_categorical(\"cnn_filters1\", [16, 32, 64]),\n",
    "        \"cnn_filters2\": trial.suggest_categorical(\"cnn_filters2\", [32, 64, 128]),\n",
    "        \"dense_units\": trial.suggest_categorical(\"dense_units\", [64, 128, 256]),\n",
    "        \"dropout_lstm\": trial.suggest_float(\"dropout_lstm\", 0.2, 0.5),\n",
    "        \"dropout_cnn\": trial.suggest_float(\"dropout_cnn\", 0.2, 0.5),\n",
    "        \"dropout_final\": trial.suggest_float(\"dropout_final\", 0.2, 0.5),\n",
    "        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-4, 1e-2),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64, 128]),\n",
    "    }\n",
    "\n",
    "    model = build_model(input_shape=X_train.shape[1:], hp=hp)\n",
    "\n",
    "    early_stopper = EarlyStopping(monitor=\"val_loss\", patience=5, restore_best_weights=True, verbose=0)\n",
    "\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=10,\n",
    "        batch_size=hp[\"batch_size\"],\n",
    "        callbacks=[\n",
    "            early_stopper,\n",
    "            TFKerasPruningCallback(trial, \"val_accuracy\")  # ðŸ’¡ pruning callback\n",
    "        ],\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    val_accuracy = max(history.history[\"val_accuracy\"])\n",
    "    return val_accuracy\n",
    "\n",
    "# -----------------------------\n",
    "# Train function w/ tuning\n",
    "# -----------------------------\n",
    "def train_hybrid_cnn_lstm(pkl_file):\n",
    "    global X_train, X_val, y_train, y_val\n",
    "\n",
    "    X, y = load_vector_data(pkl_file)\n",
    "    print(f\"Original dataset class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "\n",
    "    X_train_all, X_test, y_train_all, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Train split before balancing: {dict(zip(*np.unique(y_train_all, return_counts=True)))}\")\n",
    "    print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    # Balance training set\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    X_train_balanced = X_train_all[balanced_idx]\n",
    "    y_train_balanced = y_train_all[balanced_idx]\n",
    "    print(f\"Balanced training class distribution: {dict(zip(*np.unique(y_train_balanced, return_counts=True)))}\")\n",
    "\n",
    "    # Validation split\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_train_balanced, y_train_balanced, test_size=0.2, stratify=y_train_balanced, random_state=SEED\n",
    "    )\n",
    "    print(f\"Final training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "    print(f\"Validation class distribution: {dict(zip(*np.unique(y_val, return_counts=True)))}\")\n",
    "\n",
    "    # Run Optuna with pruning enabled\n",
    "    print(\"Running Optuna hyperparameter tuning (with pruning)...\")\n",
    "    study = optuna.create_study(\n",
    "        direction=\"maximize\",\n",
    "        pruner=optuna.pruners.MedianPruner(n_warmup_steps=5)\n",
    "    )\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best hyperparameters: {study.best_params}\")\n",
    "\n",
    "    # Retrain best model on full training (train + val)\n",
    "    best_hp = study.best_params\n",
    "    X_full = np.concatenate([X_train, X_val])\n",
    "    y_full = np.concatenate([y_train, y_val])\n",
    "    model = build_model(input_shape=X_full.shape[1:], hp=best_hp)\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=3, verbose=1)\n",
    "    early_stopper = EarlyStopping(monitor=\"loss\", patience=2, restore_best_weights=True, verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        X_full, y_full,\n",
    "        epochs=30,\n",
    "        batch_size=best_hp[\"batch_size\"],\n",
    "        callbacks=[lr_scheduler, early_stopper],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "    print(f\"FPR         : {fpr:.4f}\")\n",
    "    print(f\"FNR         : {fnr:.4f}\")\n",
    "    print(f\"Precision   : {precision:.4f}\")\n",
    "    print(f\"F1 Score    : {f1:.4f}\")\n",
    "\n",
    "# Run\n",
    "train_hybrid_cnn_lstm(\"cwe119_cgd_gadget_vectors.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bb1b4d3",
   "metadata": {},
   "source": [
    "# CWE 119: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ced5c608",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorized Code Gadget Input --> BiLSTM + Attention ------\\\n",
    "                                                            --> Concatenate --> Dense --> Output\n",
    "Vectorized Code Gadget Input --> CNN on expanded dims ----/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dcba7fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 29313, 1: 10440}\n",
      "Train split before balancing: {0: 23450, 1: 8352}\n",
      "Test set class distribution: {0: 5863, 1: 2088}\n",
      "Balanced training class distribution: {0: 8352, 1: 8352}\n",
      "Epoch 1/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 81ms/step - accuracy: 0.6364 - loss: 0.6174 - learning_rate: 0.0013\n",
      "Epoch 2/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.7741 - loss: 0.4756 - learning_rate: 0.0013\n",
      "Epoch 3/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.8150 - loss: 0.4033 - learning_rate: 0.0013\n",
      "Epoch 4/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.8368 - loss: 0.3565 - learning_rate: 0.0013\n",
      "Epoch 5/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.8510 - loss: 0.3267 - learning_rate: 0.0013\n",
      "Epoch 6/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.8613 - loss: 0.2981 - learning_rate: 0.0013\n",
      "Epoch 7/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.8733 - loss: 0.2762 - learning_rate: 0.0013\n",
      "Epoch 8/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 80ms/step - accuracy: 0.8804 - loss: 0.2646 - learning_rate: 0.0013\n",
      "Epoch 9/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 81ms/step - accuracy: 0.8840 - loss: 0.2512 - learning_rate: 0.0013\n",
      "Epoch 10/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 85ms/step - accuracy: 0.8868 - loss: 0.2428 - learning_rate: 0.0013\n",
      "Epoch 11/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.8919 - loss: 0.2280 - learning_rate: 0.0013\n",
      "Epoch 12/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.8919 - loss: 0.2278 - learning_rate: 0.0013\n",
      "Epoch 13/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.8990 - loss: 0.2162 - learning_rate: 0.0013\n",
      "Epoch 14/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.9021 - loss: 0.2109 - learning_rate: 0.0013\n",
      "Epoch 15/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.9021 - loss: 0.2096 - learning_rate: 0.0013\n",
      "Epoch 16/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.9031 - loss: 0.2064 - learning_rate: 0.0013\n",
      "Epoch 17/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.9092 - loss: 0.1957 - learning_rate: 0.0013\n",
      "Epoch 18/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.9062 - loss: 0.1975 - learning_rate: 0.0013\n",
      "Epoch 19/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.9097 - loss: 0.1937 - learning_rate: 0.0013\n",
      "Epoch 20/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.9141 - loss: 0.1855 - learning_rate: 0.0013\n",
      "Epoch 21/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 86ms/step - accuracy: 0.9141 - loss: 0.1854 - learning_rate: 0.0013\n",
      "Epoch 22/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 87ms/step - accuracy: 0.9138 - loss: 0.1794 - learning_rate: 0.0013\n",
      "Epoch 23/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 87ms/step - accuracy: 0.9147 - loss: 0.1807 - learning_rate: 0.0013\n",
      "Epoch 24/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 87ms/step - accuracy: 0.9173 - loss: 0.1764 - learning_rate: 0.0013\n",
      "Epoch 25/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 87ms/step - accuracy: 0.9192 - loss: 0.1712 - learning_rate: 0.0013\n",
      "Epoch 26/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 87ms/step - accuracy: 0.9191 - loss: 0.1721 - learning_rate: 0.0013\n",
      "Epoch 27/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 88ms/step - accuracy: 0.9191 - loss: 0.1702 - learning_rate: 0.0013\n",
      "Epoch 28/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 88ms/step - accuracy: 0.9209 - loss: 0.1701 - learning_rate: 0.0013\n",
      "Epoch 29/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 88ms/step - accuracy: 0.9214 - loss: 0.1704 - learning_rate: 0.0013\n",
      "Epoch 30/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m46s\u001b[0m 88ms/step - accuracy: 0.9255 - loss: 0.1606 - learning_rate: 0.0013\n",
      "\n",
      "Evaluating model on test set...\n",
      "Test Accuracy: 0.8380\n",
      "\u001b[1m249/249\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 52ms/step\n",
      "TPR (Recall): 0.8817\n",
      "FPR         : 0.1776\n",
      "FNR         : 0.1183\n",
      "Precision   : 0.6388\n",
      "F1 Score    : 0.7408\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Input, Conv2D, MaxPool2D, Flatten, Dense, Dropout, LeakyReLU,\n",
    "    LSTM, Bidirectional, Concatenate, Lambda\n",
    ")\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "# Set seed for reproducibility\n",
    "SEED = 41\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "# -----------------------------\n",
    "# Load vector data\n",
    "# -----------------------------\n",
    "def load_vector_data(filename):\n",
    "    df = pd.read_pickle(filename)\n",
    "    vectors = np.stack(df[\"vector\"].values).astype(np.float32)\n",
    "    labels = df[\"val\"].values.astype(int)\n",
    "    return vectors, labels\n",
    "\n",
    "# -----------------------------\n",
    "# Custom attention layer\n",
    "# -----------------------------\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "# -----------------------------\n",
    "# Build model with best hyperparams\n",
    "# -----------------------------\n",
    "def build_best_model(input_shape):\n",
    "    hp = {\n",
    "        'lstm_units': 128,\n",
    "        'cnn_filters1': 16,\n",
    "        'cnn_filters2': 32,\n",
    "        'dense_units': 128,\n",
    "        'dropout_lstm': 0.3946052251938623,\n",
    "        'dropout_cnn': 0.3900848497439904,\n",
    "        'dropout_final': 0.21345658079480226,\n",
    "        'learning_rate': 0.0012561055586646933\n",
    "    }\n",
    "\n",
    "    input_layer = Input(shape=input_shape)\n",
    "\n",
    "    x_lstm = Bidirectional(LSTM(hp[\"lstm_units\"], return_sequences=True))(input_layer)\n",
    "    x_lstm = AttentionLayer()(x_lstm)\n",
    "    x_lstm = Dense(hp[\"dense_units\"])(x_lstm)\n",
    "    x_lstm = LeakyReLU()(x_lstm)\n",
    "    x_lstm = Dropout(hp[\"dropout_lstm\"])(x_lstm)\n",
    "\n",
    "    x_cnn = Lambda(lambda x: tf.expand_dims(x, axis=-1))(input_layer)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    x_cnn = Dense(hp[\"dense_units\"], activation=\"relu\")(x_cnn)\n",
    "    x_cnn = Dropout(hp[\"dropout_cnn\"])(x_cnn)\n",
    "\n",
    "    merged = Concatenate()([x_lstm, x_cnn])\n",
    "    merged = Dense(hp[\"dense_units\"], activation=\"relu\")(merged)\n",
    "    merged = Dropout(hp[\"dropout_final\"])(merged)\n",
    "    output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=input_layer, outputs=output)\n",
    "    model.compile(optimizer=Adam(hp[\"learning_rate\"]), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "# -----------------------------\n",
    "# Training and evaluation\n",
    "# -----------------------------\n",
    "def train_final_model(pkl_file):\n",
    "    X, y = load_vector_data(pkl_file)\n",
    "    print(f\"Original dataset class distribution: {dict(zip(*np.unique(y, return_counts=True)))}\")\n",
    "\n",
    "    X_train_all, X_test, y_train_all, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, stratify=y, random_state=SEED\n",
    "    )\n",
    "    print(f\"Train split before balancing: {dict(zip(*np.unique(y_train_all, return_counts=True)))}\")\n",
    "    print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    # Balance training set (undersample class 0)\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    balanced_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(balanced_idx)\n",
    "\n",
    "    X_train = X_train_all[balanced_idx]\n",
    "    y_train = y_train_all[balanced_idx]\n",
    "    print(f\"Balanced training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "    model = build_best_model(input_shape=X_train.shape[1:])\n",
    "\n",
    "    lr_scheduler = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=3, verbose=1)\n",
    "\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30,\n",
    "        batch_size=32,\n",
    "        callbacks=[lr_scheduler],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating model on test set...\")\n",
    "    loss, acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    y_pred_probs = model.predict(X_test)\n",
    "    y_pred = (y_pred_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "    print(f\"FPR         : {fpr:.4f}\")\n",
    "    print(f\"FNR         : {fnr:.4f}\")\n",
    "    print(f\"Precision   : {precision:.4f}\")\n",
    "    print(f\"F1 Score    : {f1:.4f}\")\n",
    "\n",
    "# Run\n",
    "train_final_model(\"cwe119_cgd_gadget_vectors.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
