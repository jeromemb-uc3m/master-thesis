{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58877b26",
   "metadata": {},
   "source": [
    "# CWE 399: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab494ea1",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorized Code Gadget Input --> BiLSTM + Attention -----------\\\n",
    "                                                                |\n",
    "                                                                |--> Concatenate --> Dense --> Output\n",
    "                                                                |\n",
    "Raw Code Text Input    -->   Grayscale Image -->   CNN --------/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "72ed5305",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 14600, 1: 7285}\n",
      "Train split before balancing: {0: 11680, 1: 5828}\n",
      "Test set class distribution: {0: 2920, 1: 1457}\n",
      "Balanced training class distribution: {0: 5828, 1: 5828}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-25 20:56:35,659] A new study created in memory with name: no-name-db4a376c-ba77-4856-9adc-2c06c7f2855b\n",
      "[I 2025-05-25 20:57:15,947] Trial 0 finished with value: 0.9391080737113953 and parameters: {'lstm_units': 64, 'dense_units': 64, 'dropout_lstm': 0.2374121939836571, 'dropout_cnn': 0.27534292883328343, 'dropout_final': 0.2272902913249341, 'cnn_filters1': 32, 'cnn_filters2': 64, 'lr': 0.007900455844014219, 'batch_size': 64}. Best is trial 0 with value: 0.9391080737113953.\n",
      "[I 2025-05-25 20:58:52,872] Trial 1 finished with value: 0.9069468379020691 and parameters: {'lstm_units': 192, 'dense_units': 64, 'dropout_lstm': 0.28303036037080276, 'dropout_cnn': 0.3431597129903241, 'dropout_final': 0.32800876068097073, 'cnn_filters1': 32, 'cnn_filters2': 128, 'lr': 0.0003213687536580007, 'batch_size': 64}. Best is trial 0 with value: 0.9391080737113953.\n",
      "[I 2025-05-25 21:00:02,305] Trial 2 finished with value: 0.9215266108512878 and parameters: {'lstm_units': 128, 'dense_units': 64, 'dropout_lstm': 0.3083893069595397, 'dropout_cnn': 0.3009199058317045, 'dropout_final': 0.39352541784918404, 'cnn_filters1': 32, 'cnn_filters2': 128, 'lr': 0.0023360944893229874, 'batch_size': 64}. Best is trial 0 with value: 0.9391080737113953.\n",
      "[I 2025-05-25 21:01:30,459] Trial 3 finished with value: 0.9206689596176147 and parameters: {'lstm_units': 128, 'dense_units': 192, 'dropout_lstm': 0.3140008996891891, 'dropout_cnn': 0.2337470383794373, 'dropout_final': 0.2666546890862395, 'cnn_filters1': 64, 'cnn_filters2': 64, 'lr': 0.0008440336792018726, 'batch_size': 32}. Best is trial 0 with value: 0.9391080737113953.\n",
      "[I 2025-05-25 21:03:02,429] Trial 4 finished with value: 0.932675838470459 and parameters: {'lstm_units': 192, 'dense_units': 64, 'dropout_lstm': 0.433697867545366, 'dropout_cnn': 0.22391510233306317, 'dropout_final': 0.24796653289396806, 'cnn_filters1': 32, 'cnn_filters2': 64, 'lr': 0.007669418374146321, 'batch_size': 64}. Best is trial 0 with value: 0.9391080737113953.\n",
      "[I 2025-05-25 21:03:12,659] Trial 5 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:03:26,355] Trial 6 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 21:03:41,743] Trial 7 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:03:52,334] Trial 8 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:04:10,115] Trial 9 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:05:00,454] Trial 10 pruned. Trial was pruned at epoch 7.\n",
      "[I 2025-05-25 21:05:22,799] Trial 11 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 21:05:36,261] Trial 12 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:05:45,974] Trial 13 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:05:59,617] Trial 14 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:06:12,275] Trial 15 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 21:06:22,065] Trial 16 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:06:36,162] Trial 17 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 21:07:03,803] Trial 18 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 21:07:16,499] Trial 19 pruned. Trial was pruned at epoch 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: 0\n",
      "Best hyperparameters: {'lstm_units': 64, 'dense_units': 64, 'dropout_lstm': 0.2374121939836571, 'dropout_cnn': 0.27534292883328343, 'dropout_final': 0.2272902913249341, 'cnn_filters1': 32, 'cnn_filters2': 64, 'lr': 0.007900455844014219, 'batch_size': 64}\n",
      "Epoch 1/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 23ms/step - accuracy: 0.5647 - loss: 0.6858\n",
      "Epoch 2/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.8278 - loss: 0.3798\n",
      "Epoch 3/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.8808 - loss: 0.2701\n",
      "Epoch 4/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.8953 - loss: 0.2333\n",
      "Epoch 5/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9079 - loss: 0.2070\n",
      "Epoch 6/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9148 - loss: 0.1914\n",
      "Epoch 7/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9175 - loss: 0.1762\n",
      "Epoch 8/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9226 - loss: 0.1732\n",
      "Epoch 9/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9300 - loss: 0.1556\n",
      "Epoch 10/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9348 - loss: 0.1465\n",
      "Epoch 11/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9317 - loss: 0.1548\n",
      "Epoch 12/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9408 - loss: 0.1334\n",
      "Epoch 13/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9442 - loss: 0.1254\n",
      "Epoch 14/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9446 - loss: 0.1210\n",
      "Epoch 15/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9434 - loss: 0.1213\n",
      "Epoch 16/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9464 - loss: 0.1121\n",
      "Epoch 17/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9486 - loss: 0.1117\n",
      "Epoch 18/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9467 - loss: 0.1101\n",
      "Epoch 19/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9476 - loss: 0.1040\n",
      "Epoch 20/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.9498 - loss: 0.1050\n",
      "Epoch 21/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.9509 - loss: 0.1072\n",
      "Epoch 22/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 31ms/step - accuracy: 0.9474 - loss: 0.1089\n",
      "Epoch 23/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.9450 - loss: 0.1100\n",
      "Epoch 24/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 26ms/step - accuracy: 0.9486 - loss: 0.1028\n",
      "Epoch 25/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 27ms/step - accuracy: 0.9520 - loss: 0.1021\n",
      "Epoch 26/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 25ms/step - accuracy: 0.9544 - loss: 0.0925\n",
      "Epoch 27/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9509 - loss: 0.0989\n",
      "Epoch 28/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9539 - loss: 0.0960\n",
      "Epoch 29/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 23ms/step - accuracy: 0.9541 - loss: 0.0944\n",
      "Epoch 30/30\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9573 - loss: 0.0965\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9306 - loss: 0.1815\n",
      "Test Accuracy: 0.9376\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 10ms/step\n",
      "TPR (Recall): 0.9327\n",
      "FPR         : 0.0599\n",
      "FNR         : 0.0673\n",
      "Precision   : 0.8859\n",
      "F1 Score    : 0.9087\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from math import floor, sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Bidirectional, LSTM, LeakyReLU,\n",
    "                                     Concatenate, Conv2D, MaxPool2D, Flatten)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras import backend as K\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "SEED = 41\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "\n",
    "def load_blstm_data(filename):\n",
    "    df = pd.read_pickle(filename)\n",
    "    vectors = np.stack(df[\"vector\"].values)\n",
    "    labels = df[\"val\"].values.astype(np.int32)\n",
    "    valid_mask = (labels == 0) | (labels == 1)\n",
    "    return vectors[valid_mask], labels[valid_mask]\n",
    "\n",
    "\n",
    "def parse_file(filename):\n",
    "    codes, labels = [], []\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget, label = [], None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if label in (0, 1) and gadget:\n",
    "                    codes.append(\"\\n\".join(gadget))\n",
    "                    labels.append(label)\n",
    "                gadget, label = [], None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    value = int(stripped)\n",
    "                    label = value if value in (0, 1) else None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "        if label in (0, 1) and gadget:\n",
    "            codes.append(\"\\n\".join(gadget))\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(codes), np.array(labels)\n",
    "\n",
    "\n",
    "def code_to_image(code_sample, target_size=32):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "    padded = np.zeros((target_size, target_size), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    padded[:min(h, target_size), :min(w, target_size)] = cropped[:min(h, target_size), :min(w, target_size)]\n",
    "    padded = np.expand_dims(padded, axis=-1).astype(np.float32)\n",
    "    return resize(padded, (target_size, target_size)).numpy() / 255.0\n",
    "\n",
    "\n",
    "def load_hybrid_data(vector_pkl, code_txt):\n",
    "    blstm_X, blstm_y = load_blstm_data(vector_pkl)\n",
    "    code_texts, code_y = parse_file(code_txt)\n",
    "\n",
    "    # Filter to minimum common length to ensure sync\n",
    "    min_len = min(len(blstm_X), len(code_texts))\n",
    "    blstm_X, blstm_y = blstm_X[:min_len], blstm_y[:min_len]\n",
    "    code_texts, code_y = code_texts[:min_len], code_y[:min_len]\n",
    "\n",
    "    match_mask = (blstm_y == code_y)\n",
    "    blstm_X, labels, code_texts = blstm_X[match_mask], blstm_y[match_mask], code_texts[match_mask]\n",
    "\n",
    "    print(f\"Original dataset class distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
    "\n",
    "    X1_train_all, X1_test, codes_train_all, codes_test, y_train_all, y_test = train_test_split(\n",
    "        blstm_X, code_texts, labels, test_size=0.2, stratify=labels, random_state=SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Train split before balancing: {dict(zip(*np.unique(y_train_all, return_counts=True)))}\")\n",
    "    print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    bal_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(bal_idx)\n",
    "\n",
    "    X1_bal, codes_bal, y_bal = X1_train_all[bal_idx], codes_train_all[bal_idx], y_train_all[bal_idx]\n",
    "    print(f\"Balanced training class distribution: {dict(zip(*np.unique(y_bal, return_counts=True)))}\")\n",
    "\n",
    "    X1_train, X1_val, codes_train, codes_val, y_train, y_val = train_test_split(\n",
    "        X1_bal, codes_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X2_train = np.stack([code_to_image(c) for c in codes_train])\n",
    "    X2_val = np.stack([code_to_image(c) for c in codes_val])\n",
    "    X2_test = np.stack([code_to_image(c) for c in codes_test])\n",
    "\n",
    "    return X1_train, X1_val, X1_test, X2_train, X2_val, X2_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def build_hybrid_model(blstm_input_shape, hp, cnn_input_shape=(32, 32, 1)):\n",
    "    input_blstm = Input(shape=blstm_input_shape)\n",
    "    x_lstm = Bidirectional(LSTM(hp[\"lstm_units\"], return_sequences=True))(input_blstm)\n",
    "    x_lstm = AttentionLayer()(x_lstm)\n",
    "    x_lstm = Dense(hp[\"dense_units\"])(x_lstm)\n",
    "    x_lstm = LeakyReLU()(x_lstm)\n",
    "    x_lstm = Dropout(hp[\"dropout_lstm\"])(x_lstm)\n",
    "\n",
    "    input_cnn = Input(shape=cnn_input_shape)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(input_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    x_cnn = Dense(hp[\"dense_units\"], activation=\"relu\")(x_cnn)\n",
    "    x_cnn = Dropout(hp[\"dropout_cnn\"])(x_cnn)\n",
    "\n",
    "    merged = Concatenate()([x_lstm, x_cnn])\n",
    "    merged = Dense(hp[\"dense_units\"], activation=\"relu\")(merged)\n",
    "    merged = Dropout(hp[\"dropout_final\"])(merged)\n",
    "    output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[input_blstm, input_cnn], outputs=output)\n",
    "    model.compile(optimizer=Adam(hp[\"lr\"]), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hp = {\n",
    "        \"lstm_units\": trial.suggest_categorical(\"lstm_units\", [64, 128, 192]),\n",
    "        \"dense_units\": trial.suggest_categorical(\"dense_units\", [64, 128, 192]),\n",
    "        \"dropout_lstm\": trial.suggest_float(\"dropout_lstm\", 0.2, 0.5),\n",
    "        \"dropout_cnn\": trial.suggest_float(\"dropout_cnn\", 0.2, 0.5),\n",
    "        \"dropout_final\": trial.suggest_float(\"dropout_final\", 0.2, 0.5),\n",
    "        \"cnn_filters1\": trial.suggest_categorical(\"cnn_filters1\", [32, 64]),\n",
    "        \"cnn_filters2\": trial.suggest_categorical(\"cnn_filters2\", [64, 128]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "    }\n",
    "\n",
    "    model = build_hybrid_model(X1_train.shape[1:], hp)\n",
    "    history = model.fit(\n",
    "        [X1_train, X2_train], y_train,\n",
    "        validation_data=([X1_val, X2_val], y_val),\n",
    "        epochs=10,\n",
    "        batch_size=hp[\"batch_size\"],\n",
    "        verbose=0,\n",
    "        callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\")]\n",
    "    )\n",
    "    return max(history.history[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "def train_hybrid_with_optuna(vector_pkl, code_txt):\n",
    "    global X1_train, X1_val, X1_test, X2_train, X2_val, X2_test, y_train, y_val, y_test\n",
    "    X1_train, X1_val, X1_test, X2_train, X2_val, X2_test, y_train, y_val, y_test = load_hybrid_data(vector_pkl, code_txt)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best hyperparameters: {study.best_params}\")\n",
    "\n",
    "    best_hp = study.best_params\n",
    "    X1_full = np.concatenate([X1_train, X1_val])\n",
    "    X2_full = np.concatenate([X2_train, X2_val])\n",
    "    y_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "    model = build_hybrid_model(X1_full.shape[1:], best_hp)\n",
    "    model.fit(\n",
    "        [X1_full, X2_full], y_full,\n",
    "        epochs=30,\n",
    "        batch_size=best_hp[\"batch_size\"],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    loss, acc = model.evaluate([X1_test, X2_test], y_test)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    y_probs = model.predict([X1_test, X2_test])\n",
    "    y_pred = (y_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "    print(f\"FPR         : {fpr:.4f}\")\n",
    "    print(f\"FNR         : {fnr:.4f}\")\n",
    "    print(f\"Precision   : {precision:.4f}\")\n",
    "    print(f\"F1 Score    : {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Run\n",
    "vector_file = \"cwe399_cgd_gadget_vectors.pkl\"\n",
    "code_file = \"cwe399_cgd.txt\"\n",
    "train_hybrid_with_optuna(vector_file, code_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d224c7ae",
   "metadata": {},
   "source": [
    "# CWE 399: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29c60b4",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorized Code Gadget Input --> BiLSTM + Attention -----------\\\n",
    "                                                                |\n",
    "                                                                |--> Concatenate --> Dense --> Output\n",
    "                                                                |\n",
    "Raw Code Text Input    -->   Grayscale Image -->   CNN --------/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57428f85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 14600, 1: 7285}\n",
      "Train split before balancing: {0: 11680, 1: 5828}\n",
      "Test set class distribution: {0: 2920, 1: 1457}\n",
      "Balanced training class distribution: {0: 5828, 1: 5828}\n",
      "Epoch 1/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 47ms/step - accuracy: 0.6172 - loss: 0.6314\n",
      "Epoch 2/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 46ms/step - accuracy: 0.8662 - loss: 0.2929\n",
      "Epoch 3/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 46ms/step - accuracy: 0.8882 - loss: 0.2369\n",
      "Epoch 4/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 45ms/step - accuracy: 0.9038 - loss: 0.2103\n",
      "Epoch 5/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - accuracy: 0.9132 - loss: 0.1974\n",
      "Epoch 6/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 45ms/step - accuracy: 0.9217 - loss: 0.1721\n",
      "Epoch 7/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 46ms/step - accuracy: 0.9203 - loss: 0.1694\n",
      "Epoch 8/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - accuracy: 0.9305 - loss: 0.1513\n",
      "Epoch 9/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - accuracy: 0.9360 - loss: 0.1407\n",
      "Epoch 10/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9314 - loss: 0.1515\n",
      "Epoch 11/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9390 - loss: 0.1331\n",
      "Epoch 12/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9414 - loss: 0.1288\n",
      "Epoch 13/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 53ms/step - accuracy: 0.9421 - loss: 0.1253\n",
      "Epoch 14/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 47ms/step - accuracy: 0.9437 - loss: 0.1157\n",
      "Epoch 15/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 48ms/step - accuracy: 0.9478 - loss: 0.1115\n",
      "Epoch 16/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 54ms/step - accuracy: 0.9477 - loss: 0.1095\n",
      "Epoch 17/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 56ms/step - accuracy: 0.9506 - loss: 0.1045\n",
      "Epoch 18/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 55ms/step - accuracy: 0.9506 - loss: 0.1023\n",
      "Epoch 19/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 52ms/step - accuracy: 0.9458 - loss: 0.1185\n",
      "Epoch 20/20\n",
      "\u001b[1m183/183\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 46ms/step - accuracy: 0.9457 - loss: 0.1148\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 24ms/step - accuracy: 0.9211 - loss: 0.1651\n",
      "Test Accuracy: 0.9257\n",
      "\u001b[1m137/137\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step\n",
      "TPR (Recall): 0.9753\n",
      "FPR         : 0.0990\n",
      "FNR         : 0.0247\n",
      "Precision   : 0.8310\n",
      "F1 Score    : 0.8974\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from math import floor, sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Bidirectional, LSTM, LeakyReLU,\n",
    "                                     Concatenate, Conv2D, MaxPool2D, Flatten)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "SEED = 41\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "def load_blstm_data(filename):\n",
    "    df = pd.read_pickle(filename)\n",
    "    vectors = np.stack(df[\"vector\"].values)\n",
    "    labels = df[\"val\"].values.astype(np.int32)\n",
    "    valid_mask = (labels == 0) | (labels == 1)\n",
    "    return vectors[valid_mask], labels[valid_mask]\n",
    "\n",
    "def parse_file(filename):\n",
    "    codes, labels = [], []\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget, label = [], None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if label in (0, 1) and gadget:\n",
    "                    codes.append(\"\\n\".join(gadget))\n",
    "                    labels.append(label)\n",
    "                gadget, label = [], None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    value = int(stripped)\n",
    "                    label = value if value in (0, 1) else None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "        if label in (0, 1) and gadget:\n",
    "            codes.append(\"\\n\".join(gadget))\n",
    "            labels.append(label)\n",
    "    return np.array(codes), np.array(labels)\n",
    "\n",
    "def code_to_image(code_sample, target_size=32):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "    padded = np.zeros((target_size, target_size), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    padded[:min(h, target_size), :min(w, target_size)] = cropped[:min(h, target_size), :min(w, target_size)]\n",
    "    padded = np.expand_dims(padded, axis=-1).astype(np.float32)\n",
    "    return resize(padded, (target_size, target_size)).numpy() / 255.0\n",
    "\n",
    "def load_final_data(vector_pkl, code_txt):\n",
    "    blstm_X, blstm_y = load_blstm_data(vector_pkl)\n",
    "    code_texts, code_y = parse_file(code_txt)\n",
    "\n",
    "    min_len = min(len(blstm_X), len(code_texts))\n",
    "    blstm_X, blstm_y = blstm_X[:min_len], blstm_y[:min_len]\n",
    "    code_texts, code_y = code_texts[:min_len], code_y[:min_len]\n",
    "\n",
    "    match_mask = (blstm_y == code_y)\n",
    "    blstm_X, labels, code_texts = blstm_X[match_mask], blstm_y[match_mask], code_texts[match_mask]\n",
    "\n",
    "    print(f\"Original dataset class distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
    "\n",
    "    X1_train_all, X1_test, codes_train_all, codes_test, y_train_all, y_test = train_test_split(\n",
    "        blstm_X, code_texts, labels, test_size=0.2, stratify=labels, random_state=SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Train split before balancing: {dict(zip(*np.unique(y_train_all, return_counts=True)))}\")\n",
    "    print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    bal_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(bal_idx)\n",
    "\n",
    "    X1_train = X1_train_all[bal_idx]\n",
    "    codes_train = codes_train_all[bal_idx]\n",
    "    y_train = y_train_all[bal_idx]\n",
    "\n",
    "    print(f\"Balanced training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "    X2_train = np.stack([code_to_image(c) for c in codes_train])\n",
    "    X2_test = np.stack([code_to_image(c) for c in codes_test])\n",
    "\n",
    "    return X1_train, X2_train, y_train, X1_test, X2_test, y_test\n",
    "\n",
    "def build_final_model(blstm_input_shape, cnn_input_shape=(32, 32, 1)):\n",
    "    hp = {\n",
    "        'lstm_units': 64,\n",
    "        'dense_units': 64,\n",
    "        'dropout_lstm': 0.2374121939836571,\n",
    "        'dropout_cnn': 0.27534292883328343,\n",
    "        'dropout_final': 0.2272902913249341,\n",
    "        'cnn_filters1': 32,\n",
    "        'cnn_filters2': 64,\n",
    "        'lr': 0.007900455844014219,\n",
    "    }\n",
    "\n",
    "    input_blstm = Input(shape=blstm_input_shape)\n",
    "    x_lstm = Bidirectional(LSTM(hp[\"lstm_units\"], return_sequences=True))(input_blstm)\n",
    "    x_lstm = AttentionLayer()(x_lstm)\n",
    "    x_lstm = Dense(hp[\"dense_units\"])(x_lstm)\n",
    "    x_lstm = LeakyReLU()(x_lstm)\n",
    "    x_lstm = Dropout(hp[\"dropout_lstm\"])(x_lstm)\n",
    "\n",
    "    input_cnn = Input(shape=cnn_input_shape)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(input_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    x_cnn = Dense(hp[\"dense_units\"], activation=\"relu\")(x_cnn)\n",
    "    x_cnn = Dropout(hp[\"dropout_cnn\"])(x_cnn)\n",
    "\n",
    "    merged = Concatenate()([x_lstm, x_cnn])\n",
    "    merged = Dense(hp[\"dense_units\"], activation=\"relu\")(merged)\n",
    "    merged = Dropout(hp[\"dropout_final\"])(merged)\n",
    "    output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[input_blstm, input_cnn], outputs=output)\n",
    "    model.compile(optimizer=Adam(hp[\"lr\"]), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(vector_pkl, code_txt):\n",
    "    X1_train, X2_train, y_train, X1_test, X2_test, y_test = load_final_data(vector_pkl, code_txt)\n",
    "\n",
    "    model = build_final_model(X1_train.shape[1:])\n",
    "    model.fit([X1_train, X2_train], y_train, epochs=20, batch_size=64, verbose=1)\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    loss, acc = model.evaluate([X1_test, X2_test], y_test)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    y_probs = model.predict([X1_test, X2_test])\n",
    "    y_pred = (y_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "    print(f\"FPR         : {fpr:.4f}\")\n",
    "    print(f\"FNR         : {fnr:.4f}\")\n",
    "    print(f\"Precision   : {precision:.4f}\")\n",
    "    print(f\"F1 Score    : {f1:.4f}\")\n",
    "\n",
    "# Run\n",
    "vector_file = \"cwe399_cgd_gadget_vectors.pkl\"\n",
    "code_file = \"cwe399_cgd.txt\"\n",
    "train_and_evaluate(vector_file, code_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd4f27c",
   "metadata": {},
   "source": [
    "# CWE 119: Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7631c9",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorized Code Gadget Input --> BiLSTM + Attention -----------\\\n",
    "                                                                |\n",
    "                                                                |--> Concatenate --> Dense --> Output\n",
    "                                                                |\n",
    "Raw Code Text Input    -->   Grayscale Image -->   CNN --------/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55be6a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 29313, 1: 10440}\n",
      "Train split before balancing: {0: 23450, 1: 8352}\n",
      "Test set class distribution: {0: 5863, 1: 2088}\n",
      "Balanced training class distribution: {0: 8352, 1: 8352}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-25 21:45:49,172] A new study created in memory with name: no-name-00ba71d9-c424-44b8-9e50-badbd991b091\n",
      "[I 2025-05-25 21:50:49,194] Trial 0 finished with value: 0.8308889269828796 and parameters: {'lstm_units': 192, 'dense_units': 192, 'dropout_lstm': 0.2742058267404428, 'dropout_cnn': 0.37941882727940535, 'dropout_final': 0.4445811161973845, 'cnn_filters1': 64, 'cnn_filters2': 64, 'lr': 0.00033987384024195146, 'batch_size': 32}. Best is trial 0 with value: 0.8308889269828796.\n",
      "[I 2025-05-25 21:53:40,339] Trial 1 finished with value: 0.8548339009284973 and parameters: {'lstm_units': 128, 'dense_units': 192, 'dropout_lstm': 0.29717900056976815, 'dropout_cnn': 0.4787097987303838, 'dropout_final': 0.4239948401118443, 'cnn_filters1': 64, 'cnn_filters2': 64, 'lr': 0.00017342834914139184, 'batch_size': 32}. Best is trial 1 with value: 0.8548339009284973.\n",
      "[I 2025-05-25 21:56:42,350] Trial 2 finished with value: 0.8575276732444763 and parameters: {'lstm_units': 128, 'dense_units': 64, 'dropout_lstm': 0.44019520286998604, 'dropout_cnn': 0.42522026456908246, 'dropout_final': 0.49689209860503014, 'cnn_filters1': 64, 'cnn_filters2': 128, 'lr': 0.003948431087230644, 'batch_size': 32}. Best is trial 2 with value: 0.8575276732444763.\n",
      "[I 2025-05-25 22:01:24,537] Trial 3 finished with value: 0.8512421250343323 and parameters: {'lstm_units': 192, 'dense_units': 192, 'dropout_lstm': 0.42915150138772296, 'dropout_cnn': 0.368619567301103, 'dropout_final': 0.40196943469561197, 'cnn_filters1': 32, 'cnn_filters2': 128, 'lr': 0.0010503765671745454, 'batch_size': 64}. Best is trial 2 with value: 0.8575276732444763.\n",
      "[I 2025-05-25 22:04:37,286] Trial 4 finished with value: 0.8491469621658325 and parameters: {'lstm_units': 128, 'dense_units': 64, 'dropout_lstm': 0.4333840058804724, 'dropout_cnn': 0.45455068005934446, 'dropout_final': 0.3569524844600594, 'cnn_filters1': 64, 'cnn_filters2': 128, 'lr': 0.006300243744671765, 'batch_size': 32}. Best is trial 2 with value: 0.8575276732444763.\n",
      "[I 2025-05-25 22:04:59,716] Trial 5 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 22:05:40,715] Trial 6 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 22:06:23,931] Trial 7 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 22:06:54,595] Trial 8 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 22:07:29,972] Trial 9 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 22:08:22,863] Trial 10 pruned. Trial was pruned at epoch 3.\n",
      "[I 2025-05-25 22:09:13,449] Trial 11 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 22:10:16,663] Trial 12 pruned. Trial was pruned at epoch 2.\n",
      "[I 2025-05-25 22:10:40,276] Trial 13 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 22:11:26,533] Trial 14 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 22:11:58,614] Trial 15 pruned. Trial was pruned at epoch 0.\n",
      "[I 2025-05-25 22:12:54,953] Trial 16 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 22:14:29,519] Trial 17 pruned. Trial was pruned at epoch 3.\n",
      "[I 2025-05-25 22:14:59,890] Trial 18 pruned. Trial was pruned at epoch 1.\n",
      "[I 2025-05-25 22:15:43,193] Trial 19 pruned. Trial was pruned at epoch 0.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best trial: 2\n",
      "Best hyperparameters: {'lstm_units': 128, 'dense_units': 64, 'dropout_lstm': 0.44019520286998604, 'dropout_cnn': 0.42522026456908246, 'dropout_final': 0.49689209860503014, 'cnn_filters1': 64, 'cnn_filters2': 128, 'lr': 0.003948431087230644, 'batch_size': 32}\n",
      "Epoch 1/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 44ms/step - accuracy: 0.6205 - loss: 0.6461\n",
      "Epoch 2/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.7288 - loss: 0.5399\n",
      "Epoch 3/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 45ms/step - accuracy: 0.7670 - loss: 0.4852\n",
      "Epoch 4/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 43ms/step - accuracy: 0.8096 - loss: 0.4172\n",
      "Epoch 5/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.8178 - loss: 0.3876\n",
      "Epoch 6/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.8281 - loss: 0.3537\n",
      "Epoch 7/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 44ms/step - accuracy: 0.8423 - loss: 0.3321\n",
      "Epoch 8/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.8491 - loss: 0.3166\n",
      "Epoch 9/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.8608 - loss: 0.3012\n",
      "Epoch 10/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 41ms/step - accuracy: 0.8672 - loss: 0.2892\n",
      "Epoch 11/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.8727 - loss: 0.2783\n",
      "Epoch 12/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.8777 - loss: 0.2667\n",
      "Epoch 13/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.8800 - loss: 0.2644\n",
      "Epoch 14/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.8799 - loss: 0.2612\n",
      "Epoch 15/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.8875 - loss: 0.2505\n",
      "Epoch 16/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.8847 - loss: 0.2487\n",
      "Epoch 17/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 41ms/step - accuracy: 0.8916 - loss: 0.2436\n",
      "Epoch 18/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.8933 - loss: 0.2346\n",
      "Epoch 19/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.8928 - loss: 0.2347\n",
      "Epoch 20/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.8975 - loss: 0.2238\n",
      "Epoch 21/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.9008 - loss: 0.2211\n",
      "Epoch 22/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 40ms/step - accuracy: 0.9014 - loss: 0.2186\n",
      "Epoch 23/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.9048 - loss: 0.2143\n",
      "Epoch 24/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.9047 - loss: 0.2176\n",
      "Epoch 25/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 43ms/step - accuracy: 0.9115 - loss: 0.2025\n",
      "Epoch 26/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 43ms/step - accuracy: 0.9113 - loss: 0.1993\n",
      "Epoch 27/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.9110 - loss: 0.1984\n",
      "Epoch 28/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.9137 - loss: 0.1952\n",
      "Epoch 29/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 42ms/step - accuracy: 0.9126 - loss: 0.1984\n",
      "Epoch 30/30\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 44ms/step - accuracy: 0.9109 - loss: 0.1947\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 24ms/step - accuracy: 0.8530 - loss: 0.3147\n",
      "Test Accuracy: 0.8467\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 25ms/step\n",
      "TPR (Recall): 0.9516\n",
      "FPR         : 0.1907\n",
      "FNR         : 0.0484\n",
      "Precision   : 0.6399\n",
      "F1 Score    : 0.7653\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from math import floor, sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Bidirectional, LSTM, LeakyReLU,\n",
    "                                     Concatenate, Conv2D, MaxPool2D, Flatten)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras import backend as K\n",
    "import optuna\n",
    "from optuna.integration import TFKerasPruningCallback\n",
    "\n",
    "SEED = 41\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "\n",
    "def load_blstm_data(filename):\n",
    "    df = pd.read_pickle(filename)\n",
    "    vectors = np.stack(df[\"vector\"].values)\n",
    "    labels = df[\"val\"].values.astype(np.int32)\n",
    "    valid_mask = (labels == 0) | (labels == 1)\n",
    "    return vectors[valid_mask], labels[valid_mask]\n",
    "\n",
    "\n",
    "def parse_file(filename):\n",
    "    codes, labels = [], []\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget, label = [], None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if label in (0, 1) and gadget:\n",
    "                    codes.append(\"\\n\".join(gadget))\n",
    "                    labels.append(label)\n",
    "                gadget, label = [], None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    value = int(stripped)\n",
    "                    label = value if value in (0, 1) else None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "        if label in (0, 1) and gadget:\n",
    "            codes.append(\"\\n\".join(gadget))\n",
    "            labels.append(label)\n",
    "\n",
    "    return np.array(codes), np.array(labels)\n",
    "\n",
    "\n",
    "def code_to_image(code_sample, target_size=32):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "    padded = np.zeros((target_size, target_size), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    padded[:min(h, target_size), :min(w, target_size)] = cropped[:min(h, target_size), :min(w, target_size)]\n",
    "    padded = np.expand_dims(padded, axis=-1).astype(np.float32)\n",
    "    return resize(padded, (target_size, target_size)).numpy() / 255.0\n",
    "\n",
    "\n",
    "def load_hybrid_data(vector_pkl, code_txt):\n",
    "    blstm_X, blstm_y = load_blstm_data(vector_pkl)\n",
    "    code_texts, code_y = parse_file(code_txt)\n",
    "\n",
    "    # Filter to minimum common length to ensure sync\n",
    "    min_len = min(len(blstm_X), len(code_texts))\n",
    "    blstm_X, blstm_y = blstm_X[:min_len], blstm_y[:min_len]\n",
    "    code_texts, code_y = code_texts[:min_len], code_y[:min_len]\n",
    "\n",
    "    match_mask = (blstm_y == code_y)\n",
    "    blstm_X, labels, code_texts = blstm_X[match_mask], blstm_y[match_mask], code_texts[match_mask]\n",
    "\n",
    "    print(f\"Original dataset class distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
    "\n",
    "    X1_train_all, X1_test, codes_train_all, codes_test, y_train_all, y_test = train_test_split(\n",
    "        blstm_X, code_texts, labels, test_size=0.2, stratify=labels, random_state=SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Train split before balancing: {dict(zip(*np.unique(y_train_all, return_counts=True)))}\")\n",
    "    print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    bal_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(bal_idx)\n",
    "\n",
    "    X1_bal, codes_bal, y_bal = X1_train_all[bal_idx], codes_train_all[bal_idx], y_train_all[bal_idx]\n",
    "    print(f\"Balanced training class distribution: {dict(zip(*np.unique(y_bal, return_counts=True)))}\")\n",
    "\n",
    "    X1_train, X1_val, codes_train, codes_val, y_train, y_val = train_test_split(\n",
    "        X1_bal, codes_bal, y_bal, test_size=0.2, stratify=y_bal, random_state=SEED\n",
    "    )\n",
    "\n",
    "    X2_train = np.stack([code_to_image(c) for c in codes_train])\n",
    "    X2_val = np.stack([code_to_image(c) for c in codes_val])\n",
    "    X2_test = np.stack([code_to_image(c) for c in codes_test])\n",
    "\n",
    "    return X1_train, X1_val, X1_test, X2_train, X2_val, X2_test, y_train, y_val, y_test\n",
    "\n",
    "\n",
    "def build_hybrid_model(blstm_input_shape, hp, cnn_input_shape=(32, 32, 1)):\n",
    "    input_blstm = Input(shape=blstm_input_shape)\n",
    "    x_lstm = Bidirectional(LSTM(hp[\"lstm_units\"], return_sequences=True))(input_blstm)\n",
    "    x_lstm = AttentionLayer()(x_lstm)\n",
    "    x_lstm = Dense(hp[\"dense_units\"])(x_lstm)\n",
    "    x_lstm = LeakyReLU()(x_lstm)\n",
    "    x_lstm = Dropout(hp[\"dropout_lstm\"])(x_lstm)\n",
    "\n",
    "    input_cnn = Input(shape=cnn_input_shape)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(input_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    x_cnn = Dense(hp[\"dense_units\"], activation=\"relu\")(x_cnn)\n",
    "    x_cnn = Dropout(hp[\"dropout_cnn\"])(x_cnn)\n",
    "\n",
    "    merged = Concatenate()([x_lstm, x_cnn])\n",
    "    merged = Dense(hp[\"dense_units\"], activation=\"relu\")(merged)\n",
    "    merged = Dropout(hp[\"dropout_final\"])(merged)\n",
    "    output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[input_blstm, input_cnn], outputs=output)\n",
    "    model.compile(optimizer=Adam(hp[\"lr\"]), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hp = {\n",
    "        \"lstm_units\": trial.suggest_categorical(\"lstm_units\", [64, 128, 192]),\n",
    "        \"dense_units\": trial.suggest_categorical(\"dense_units\", [64, 128, 192]),\n",
    "        \"dropout_lstm\": trial.suggest_float(\"dropout_lstm\", 0.2, 0.5),\n",
    "        \"dropout_cnn\": trial.suggest_float(\"dropout_cnn\", 0.2, 0.5),\n",
    "        \"dropout_final\": trial.suggest_float(\"dropout_final\", 0.2, 0.5),\n",
    "        \"cnn_filters1\": trial.suggest_categorical(\"cnn_filters1\", [32, 64]),\n",
    "        \"cnn_filters2\": trial.suggest_categorical(\"cnn_filters2\", [64, 128]),\n",
    "        \"lr\": trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True),\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [32, 64])\n",
    "    }\n",
    "\n",
    "    model = build_hybrid_model(X1_train.shape[1:], hp)\n",
    "    history = model.fit(\n",
    "        [X1_train, X2_train], y_train,\n",
    "        validation_data=([X1_val, X2_val], y_val),\n",
    "        epochs=10,\n",
    "        batch_size=hp[\"batch_size\"],\n",
    "        verbose=0,\n",
    "        callbacks=[TFKerasPruningCallback(trial, \"val_accuracy\")]\n",
    "    )\n",
    "    return max(history.history[\"val_accuracy\"])\n",
    "\n",
    "\n",
    "def train_hybrid_with_optuna(vector_pkl, code_txt):\n",
    "    global X1_train, X1_val, X1_test, X2_train, X2_val, X2_test, y_train, y_val, y_test\n",
    "    X1_train, X1_val, X1_test, X2_train, X2_val, X2_test, y_train, y_val, y_test = load_hybrid_data(vector_pkl, code_txt)\n",
    "\n",
    "    study = optuna.create_study(direction=\"maximize\", pruner=optuna.pruners.MedianPruner())\n",
    "    study.optimize(objective, n_trials=20)\n",
    "\n",
    "    print(f\"Best trial: {study.best_trial.number}\")\n",
    "    print(f\"Best hyperparameters: {study.best_params}\")\n",
    "\n",
    "    best_hp = study.best_params\n",
    "    X1_full = np.concatenate([X1_train, X1_val])\n",
    "    X2_full = np.concatenate([X2_train, X2_val])\n",
    "    y_full = np.concatenate([y_train, y_val])\n",
    "\n",
    "    model = build_hybrid_model(X1_full.shape[1:], best_hp)\n",
    "    model.fit(\n",
    "        [X1_full, X2_full], y_full,\n",
    "        epochs=30,\n",
    "        batch_size=best_hp[\"batch_size\"],\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    loss, acc = model.evaluate([X1_test, X2_test], y_test)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    y_probs = model.predict([X1_test, X2_test])\n",
    "    y_pred = (y_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "    print(f\"FPR         : {fpr:.4f}\")\n",
    "    print(f\"FNR         : {fnr:.4f}\")\n",
    "    print(f\"Precision   : {precision:.4f}\")\n",
    "    print(f\"F1 Score    : {f1:.4f}\")\n",
    "\n",
    "\n",
    "# Run\n",
    "vector_file = \"cwe119_cgd_gadget_vectors.pkl\"\n",
    "code_file = \"cwe119_cgd.txt\"\n",
    "train_hybrid_with_optuna(vector_file, code_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b516730a",
   "metadata": {},
   "source": [
    "# CWE 119: Final Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ec9e2b6",
   "metadata": {},
   "source": [
    "```\n",
    "Vectorized Code Gadget Input --> BiLSTM + Attention -----------\\\n",
    "                                                                |\n",
    "                                                                |--> Concatenate --> Dense --> Output\n",
    "                                                                |\n",
    "Raw Code Text Input    -->   Grayscale Image -->   CNN --------/\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e104551a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset class distribution: {0: 29313, 1: 10440}\n",
      "Train split before balancing: {0: 23450, 1: 8352}\n",
      "Test set class distribution: {0: 5863, 1: 2088}\n",
      "Balanced training class distribution: {0: 8352, 1: 8352}\n",
      "Epoch 1/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 60ms/step - accuracy: 0.6056 - loss: 0.6597\n",
      "Epoch 2/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 56ms/step - accuracy: 0.7250 - loss: 0.5531\n",
      "Epoch 3/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - accuracy: 0.7572 - loss: 0.5042\n",
      "Epoch 4/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 58ms/step - accuracy: 0.7922 - loss: 0.4397\n",
      "Epoch 5/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 55ms/step - accuracy: 0.8300 - loss: 0.3680\n",
      "Epoch 6/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 61ms/step - accuracy: 0.8450 - loss: 0.3426\n",
      "Epoch 7/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 57ms/step - accuracy: 0.8579 - loss: 0.3153\n",
      "Epoch 8/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 56ms/step - accuracy: 0.8618 - loss: 0.2988\n",
      "Epoch 9/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 55ms/step - accuracy: 0.8702 - loss: 0.2837\n",
      "Epoch 10/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 48ms/step - accuracy: 0.8700 - loss: 0.2818\n",
      "Epoch 11/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 48ms/step - accuracy: 0.8778 - loss: 0.2683\n",
      "Epoch 12/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 48ms/step - accuracy: 0.8806 - loss: 0.2584\n",
      "Epoch 13/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.8828 - loss: 0.2594\n",
      "Epoch 14/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 52ms/step - accuracy: 0.8839 - loss: 0.2497\n",
      "Epoch 15/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 51ms/step - accuracy: 0.8804 - loss: 0.2599\n",
      "Epoch 16/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 52ms/step - accuracy: 0.8850 - loss: 0.2522\n",
      "Epoch 17/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 52ms/step - accuracy: 0.8852 - loss: 0.2492\n",
      "Epoch 18/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 54ms/step - accuracy: 0.8862 - loss: 0.2460\n",
      "Epoch 19/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 53ms/step - accuracy: 0.8884 - loss: 0.2381\n",
      "Epoch 20/20\n",
      "\u001b[1m522/522\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 53ms/step - accuracy: 0.8916 - loss: 0.2331\n",
      "\n",
      "Evaluating on test set...\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 29ms/step - accuracy: 0.8441 - loss: 0.3329\n",
      "Test Accuracy: 0.8414\n",
      "\u001b[1m249/249\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 30ms/step\n",
      "TPR (Recall): 0.9114\n",
      "FPR         : 0.1835\n",
      "FNR         : 0.0886\n",
      "Precision   : 0.6388\n",
      "F1 Score    : 0.7511\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from math import floor, sqrt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (Input, Dense, Dropout, Bidirectional, LSTM, LeakyReLU,\n",
    "                                     Concatenate, Conv2D, MaxPool2D, Flatten)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.image import resize\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "SEED = 41\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "tf.random.set_seed(SEED)\n",
    "tf.config.experimental.enable_op_determinism()\n",
    "\n",
    "class AttentionLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        super(AttentionLayer, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.W = self.add_weight(name=\"att_weight\", shape=(input_shape[-1], 1),\n",
    "                                 initializer=\"normal\", trainable=True)\n",
    "        self.b = self.add_weight(name=\"att_bias\", shape=(input_shape[1], 1),\n",
    "                                 initializer=\"zeros\", trainable=True)\n",
    "        super(AttentionLayer, self).build(input_shape)\n",
    "\n",
    "    def call(self, x):\n",
    "        e = K.tanh(K.dot(x, self.W) + self.b)\n",
    "        e = K.squeeze(e, axis=-1)\n",
    "        alpha = K.softmax(e)\n",
    "        alpha = K.expand_dims(alpha, axis=-1)\n",
    "        context = x * alpha\n",
    "        context = K.sum(context, axis=1)\n",
    "        return context\n",
    "\n",
    "def load_blstm_data(filename):\n",
    "    df = pd.read_pickle(filename)\n",
    "    vectors = np.stack(df[\"vector\"].values)\n",
    "    labels = df[\"val\"].values.astype(np.int32)\n",
    "    valid_mask = (labels == 0) | (labels == 1)\n",
    "    return vectors[valid_mask], labels[valid_mask]\n",
    "\n",
    "def parse_file(filename):\n",
    "    codes, labels = [], []\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget, label = [], None\n",
    "        for line in file:\n",
    "            stripped = line.strip()\n",
    "            if not stripped:\n",
    "                continue\n",
    "            if '-' * 10 in stripped:\n",
    "                if label in (0, 1) and gadget:\n",
    "                    codes.append(\"\\n\".join(gadget))\n",
    "                    labels.append(label)\n",
    "                gadget, label = [], None\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if stripped.isdigit():\n",
    "                    value = int(stripped)\n",
    "                    label = value if value in (0, 1) else None\n",
    "                else:\n",
    "                    gadget.append(stripped)\n",
    "            else:\n",
    "                gadget.append(stripped)\n",
    "        if label in (0, 1) and gadget:\n",
    "            codes.append(\"\\n\".join(gadget))\n",
    "            labels.append(label)\n",
    "    return np.array(codes), np.array(labels)\n",
    "\n",
    "def code_to_image(code_sample, target_size=32):\n",
    "    byte_array = bytearray(code_sample, 'utf-8')\n",
    "    flat = np.array(byte_array, dtype=np.uint8)\n",
    "    size = floor(sqrt(len(flat)))\n",
    "    cropped = flat[:size * size].reshape((size, size))\n",
    "    padded = np.zeros((target_size, target_size), dtype=np.uint8)\n",
    "    h, w = cropped.shape\n",
    "    padded[:min(h, target_size), :min(w, target_size)] = cropped[:min(h, target_size), :min(w, target_size)]\n",
    "    padded = np.expand_dims(padded, axis=-1).astype(np.float32)\n",
    "    return resize(padded, (target_size, target_size)).numpy() / 255.0\n",
    "\n",
    "def load_final_data(vector_pkl, code_txt):\n",
    "    blstm_X, blstm_y = load_blstm_data(vector_pkl)\n",
    "    code_texts, code_y = parse_file(code_txt)\n",
    "\n",
    "    min_len = min(len(blstm_X), len(code_texts))\n",
    "    blstm_X, blstm_y = blstm_X[:min_len], blstm_y[:min_len]\n",
    "    code_texts, code_y = code_texts[:min_len], code_y[:min_len]\n",
    "\n",
    "    match_mask = (blstm_y == code_y)\n",
    "    blstm_X, labels, code_texts = blstm_X[match_mask], blstm_y[match_mask], code_texts[match_mask]\n",
    "\n",
    "    print(f\"Original dataset class distribution: {dict(zip(*np.unique(labels, return_counts=True)))}\")\n",
    "\n",
    "    X1_train_all, X1_test, codes_train_all, codes_test, y_train_all, y_test = train_test_split(\n",
    "        blstm_X, code_texts, labels, test_size=0.2, stratify=labels, random_state=SEED\n",
    "    )\n",
    "\n",
    "    print(f\"Train split before balancing: {dict(zip(*np.unique(y_train_all, return_counts=True)))}\")\n",
    "    print(f\"Test set class distribution: {dict(zip(*np.unique(y_test, return_counts=True)))}\")\n",
    "\n",
    "    pos_idx = np.where(y_train_all == 1)[0]\n",
    "    neg_idx = np.where(y_train_all == 0)[0]\n",
    "    rng = np.random.default_rng(SEED)\n",
    "    neg_sample = rng.choice(neg_idx, size=len(pos_idx), replace=False)\n",
    "    bal_idx = np.concatenate([pos_idx, neg_sample])\n",
    "    rng.shuffle(bal_idx)\n",
    "\n",
    "    X1_train = X1_train_all[bal_idx]\n",
    "    codes_train = codes_train_all[bal_idx]\n",
    "    y_train = y_train_all[bal_idx]\n",
    "\n",
    "    print(f\"Balanced training class distribution: {dict(zip(*np.unique(y_train, return_counts=True)))}\")\n",
    "\n",
    "    X2_train = np.stack([code_to_image(c) for c in codes_train])\n",
    "    X2_test = np.stack([code_to_image(c) for c in codes_test])\n",
    "\n",
    "    return X1_train, X2_train, y_train, X1_test, X2_test, y_test\n",
    "\n",
    "def build_final_model(blstm_input_shape, cnn_input_shape=(32, 32, 1)):\n",
    "    hp = {\n",
    "        'lstm_units': 128,\n",
    "        'dense_units': 64,\n",
    "        'dropout_lstm': 0.44019520286998604,\n",
    "        'dropout_cnn': 0.42522026456908246,\n",
    "        'dropout_final': 0.49689209860503014,\n",
    "        'cnn_filters1': 64,\n",
    "        'cnn_filters2': 128,\n",
    "        'lr': 0.003948431087230644,\n",
    "    }\n",
    "\n",
    "    input_blstm = Input(shape=blstm_input_shape)\n",
    "    x_lstm = Bidirectional(LSTM(hp[\"lstm_units\"], return_sequences=True))(input_blstm)\n",
    "    x_lstm = AttentionLayer()(x_lstm)\n",
    "    x_lstm = Dense(hp[\"dense_units\"])(x_lstm)\n",
    "    x_lstm = LeakyReLU()(x_lstm)\n",
    "    x_lstm = Dropout(hp[\"dropout_lstm\"])(x_lstm)\n",
    "\n",
    "    input_cnn = Input(shape=cnn_input_shape)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters1\"], (3, 3), padding=\"same\", activation=\"relu\")(input_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Conv2D(hp[\"cnn_filters2\"], (3, 3), padding=\"same\", activation=\"relu\")(x_cnn)\n",
    "    x_cnn = MaxPool2D()(x_cnn)\n",
    "    x_cnn = Flatten()(x_cnn)\n",
    "    x_cnn = Dense(hp[\"dense_units\"], activation=\"relu\")(x_cnn)\n",
    "    x_cnn = Dropout(hp[\"dropout_cnn\"])(x_cnn)\n",
    "\n",
    "    merged = Concatenate()([x_lstm, x_cnn])\n",
    "    merged = Dense(hp[\"dense_units\"], activation=\"relu\")(merged)\n",
    "    merged = Dropout(hp[\"dropout_final\"])(merged)\n",
    "    output = Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = Model(inputs=[input_blstm, input_cnn], outputs=output)\n",
    "    model.compile(optimizer=Adam(hp[\"lr\"]), loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "    return model\n",
    "\n",
    "def train_and_evaluate(vector_pkl, code_txt):\n",
    "    X1_train, X2_train, y_train, X1_test, X2_test, y_test = load_final_data(vector_pkl, code_txt)\n",
    "\n",
    "    model = build_final_model(X1_train.shape[1:])\n",
    "    model.fit([X1_train, X2_train], y_train, epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "    print(\"\\nEvaluating on test set...\")\n",
    "    loss, acc = model.evaluate([X1_test, X2_test], y_test)\n",
    "    print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "    y_probs = model.predict([X1_test, X2_test])\n",
    "    y_pred = (y_probs > 0.5).astype(int).flatten()\n",
    "\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    tpr = recall_score(y_test, y_pred)\n",
    "    precision = precision_score(y_test, y_pred)\n",
    "    f1 = f1_score(y_test, y_pred)\n",
    "    fpr = fp / (fp + tn)\n",
    "    fnr = fn / (fn + tp)\n",
    "\n",
    "    print(f\"TPR (Recall): {tpr:.4f}\")\n",
    "    print(f\"FPR         : {fpr:.4f}\")\n",
    "    print(f\"FNR         : {fnr:.4f}\")\n",
    "    print(f\"Precision   : {precision:.4f}\")\n",
    "    print(f\"F1 Score    : {f1:.4f}\")\n",
    "\n",
    "# Run\n",
    "vector_file = \"cwe119_cgd_gadget_vectors.pkl\"\n",
    "code_file = \"cwe119_cgd.txt\"\n",
    "train_and_evaluate(vector_file, code_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
