{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. IMPORT LIBRARIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. CLEAN GADGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Processes code lines by:\n",
    "\n",
    "- Replacing user-defined function names with symbolic names like `FUN1, FUN2`, etc.\n",
    "- Replacing user-defined variable names with symbolic names like `VAR1, VAR2`, etc.\n",
    "- Leaving keywords, built-in functions, and standard arguments (`argc, argv`) untouched.\n",
    "- Skipping over string and character literals, comments, and non-ASCII characters to focus only on code identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Immutable set of keywords up to C11 and C++17 standards.\n",
    "# These are reserved words in C/C++ that should not be renamed or replaced.\n",
    "keywords = frozenset({\n",
    "    '__asm', '__builtin', '__cdecl', '__declspec', '__except', '__export', '__far16', '__far32',\n",
    "    '__fastcall', '__finally', '__import', '__inline', '__int16', '__int32', '__int64', '__int8',\n",
    "    '__leave', '__optlink', '__packed', '__pascal', '__stdcall', '__system', '__thread', '__try',\n",
    "    '__unaligned', '_asm', '_Builtin', '_Cdecl', '_declspec', '_except', '_Export', '_Far16',\n",
    "    '_Far32', '_Fastcall', '_finally', '_Import', '_inline', '_int16', '_int32', '_int64',\n",
    "    '_int8', '_leave', '_Optlink', '_Packed', '_Pascal', '_stdcall', '_System', '_try', 'alignas',\n",
    "    'alignof', 'and', 'and_eq', 'asm', 'auto', 'bitand', 'bitor', 'bool', 'break', 'case',\n",
    "    'catch', 'char', 'char16_t', 'char32_t', 'class', 'compl', 'const', 'const_cast', 'constexpr',\n",
    "    'continue', 'decltype', 'default', 'delete', 'do', 'double', 'dynamic_cast', 'else', 'enum',\n",
    "    'explicit', 'export', 'extern', 'false', 'final', 'float', 'for', 'friend', 'goto', 'if',\n",
    "    'inline', 'int', 'long', 'mutable', 'namespace', 'new', 'noexcept', 'not', 'not_eq', 'nullptr',\n",
    "    'operator', 'or', 'or_eq', 'override', 'private', 'protected', 'public', 'register',\n",
    "    'reinterpret_cast', 'return', 'short', 'signed', 'sizeof', 'static', 'static_assert',\n",
    "    'static_cast', 'struct', 'switch', 'template', 'this', 'thread_local', 'throw', 'true', 'try',\n",
    "    'typedef', 'typeid', 'typename', 'union', 'unsigned', 'using', 'virtual', 'void', 'volatile',\n",
    "    'wchar_t', 'while', 'xor', 'xor_eq', 'NULL'\n",
    "})\n",
    "\n",
    "# Known, non-user-defined function names that shouldn't be replaced.\n",
    "main_set = frozenset({'main'})\n",
    "\n",
    "# Common arguments in the 'main' function that should not be renamed.\n",
    "main_args = frozenset({'argc', 'argv'})\n",
    "\n",
    "# Function to process and anonymize a C/C++ code snippet.\n",
    "# Input: gadget (list of strings), where each string is a line of code.\n",
    "# Output: cleaned_gadget (list of strings) with function/variable names replaced by symbolic names.\n",
    "def clean_gadget(gadget):\n",
    "    # Maps user-defined function names to anonymized symbols (e.g., FUN1, FUN2).\n",
    "    fun_symbols = {}\n",
    "\n",
    "    # Maps user-defined variable names to anonymized symbols (e.g., VAR1, VAR2).\n",
    "    var_symbols = {}\n",
    "\n",
    "    # Counters to generate unique symbolic names.\n",
    "    fun_count = 1\n",
    "    var_count = 1\n",
    "\n",
    "    # Regex to detect if a line is ending a multi-line comment.\n",
    "    rx_comment = re.compile(r'\\*/\\s*$')\n",
    "\n",
    "    # Regex to find candidate function names (words followed by an opening parenthesis).\n",
    "    rx_fun = re.compile(r'\\b([_A-Za-z]\\w*)\\b(?=\\s*\\()')\n",
    "\n",
    "    # Regex to find candidate variable names.\n",
    "    # Matches identifiers not immediately followed by a '('.\n",
    "    rx_var = re.compile(r'\\b([_A-Za-z]\\w*)\\b(?:(?=\\s*\\w+\\()|(?!\\s*\\w+))(?!\\s*\\()')\n",
    "\n",
    "    # List to store the cleaned code lines.\n",
    "    cleaned_gadget = []\n",
    "\n",
    "    # Process each line of the input gadget.\n",
    "    for line in gadget:\n",
    "        # Skip lines that end multi-line comments.\n",
    "        if rx_comment.search(line) is None:\n",
    "\n",
    "            # Step 1: Clean the line of literals and non-ASCII characters.\n",
    "\n",
    "            # Remove string literals (content inside double quotes) to avoid replacing names inside them.\n",
    "            nostrlit_line = re.sub(r'\".*?\"', '\"\"', line)\n",
    "\n",
    "            # Remove character literals (content inside single quotes).\n",
    "            nocharlit_line = re.sub(r\"'.*?'\", \"''\", nostrlit_line)\n",
    "\n",
    "            # Remove non-ASCII characters to ensure processing of clean ASCII text.\n",
    "            ascii_line = re.sub(r'[^\\x00-\\x7f]', r'', nocharlit_line)\n",
    "\n",
    "            # Step 2: Extract potential function and variable names.\n",
    "\n",
    "            # Find all function-like identifiers (names followed by '(').\n",
    "            user_fun = rx_fun.findall(ascii_line)\n",
    "\n",
    "            # Find all variable-like identifiers (names not followed by '(').\n",
    "            user_var = rx_var.findall(ascii_line)\n",
    "\n",
    "            # Step 3: Replace user-defined function names with symbolic names.\n",
    "            for fun_name in user_fun:\n",
    "                # Skip if it's 'main' or a reserved keyword.\n",
    "                if fun_name not in main_set and fun_name not in keywords:\n",
    "\n",
    "                    # If the function isn't already mapped, assign it a new symbol.\n",
    "                    if fun_name not in fun_symbols:\n",
    "                        fun_symbols[fun_name] = 'FUN' + str(fun_count)\n",
    "                        fun_count += 1\n",
    "\n",
    "                    # Replace function calls with the symbolic name (positive lookahead ensures it's only a function call).\n",
    "                    ascii_line = re.sub(\n",
    "                        r'\\b(' + re.escape(fun_name) + r')\\b(?=\\s*\\()',\n",
    "                        fun_symbols[fun_name],\n",
    "                        ascii_line\n",
    "                    )\n",
    "\n",
    "            # Step 4: Replace user-defined variable names with symbolic names.\n",
    "            for var_name in user_var:\n",
    "                # Skip if it's a reserved keyword or a common 'main' argument.\n",
    "                if var_name not in keywords and var_name not in main_args:\n",
    "\n",
    "                    # If the variable isn't already mapped, assign it a new symbol.\n",
    "                    if var_name not in var_symbols:\n",
    "                        var_symbols[var_name] = 'VAR' + str(var_count)\n",
    "                        var_count += 1\n",
    "\n",
    "                    # Replace variables with the symbolic name.\n",
    "                    # Uses lookaheads to ensure it's not a function name.\n",
    "                    ascii_line = re.sub(\n",
    "                        r'\\b(' + re.escape(var_name) + r')\\b(?:(?=\\s*\\w+\\()|(?!\\s*\\w+))(?!\\s*\\()',\n",
    "                        var_symbols[var_name],\n",
    "                        ascii_line\n",
    "                    )\n",
    "\n",
    "            # Append the processed line to the result list.\n",
    "            cleaned_gadget.append(ascii_line)\n",
    "\n",
    "    # Return the fully cleaned and anonymized code.\n",
    "    return cleaned_gadget\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FUNCTION TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['int VAR1 = 0', 'VAR1 = VAR1 + 1;']\n",
      "['int FUN1(int VAR1, int VAR2) {return VAR1 + VAR2;}', 'int VAR3 = FUN1(5, 10);']\n",
      "['FUN1(File VAR1, Buffer VAR2)']\n"
     ]
    }
   ],
   "source": [
    "test_gadget = ['int counter = 0', 'counter = counter + 1;']\n",
    "\n",
    "test_gadget2 = ['int sum(int a, int b) {return a + b;}', 'int result = sum(5, 10);']\n",
    "\n",
    "test_gadget3 = ['function(File file, Buffer buff)', 'this is a comment test */']\n",
    "\n",
    "print(clean_gadget(test_gadget))\n",
    "print(clean_gadget(test_gadget2))\n",
    "print(clean_gadget(test_gadget3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PARSE FILE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_file(filename):\n",
    "    \"\"\"\n",
    "    Opens and reads the specified gadget file line by line.\n",
    "    Groups lines into individual gadgets, ignoring the first \"index\" line of each gadget.\n",
    "    Cleans each gadget using `clean_gadget()`, which anonymizes variable and function names.\n",
    "    Yields a tuple for each gadget: (cleaned_gadget_lines, gadget_label)\n",
    "    \"\"\"\n",
    "    with open(filename, \"r\", encoding=\"utf8\") as file:\n",
    "        gadget = []       # Stores code lines for the current gadget\n",
    "        gadget_val = 0    # Stores the vulnerability label (0 or 1) for the current gadget\n",
    "        \n",
    "        for line in file:\n",
    "            stripped = line.strip()   # Remove leading/trailing whitespace\n",
    "            \n",
    "            if not stripped:\n",
    "                # Skip empty lines\n",
    "                continue\n",
    "\n",
    "            # Check for end-of-gadget delimiter (a line of dashes)\n",
    "            if \"-\" * 33 in line and gadget: \n",
    "                # Yield the current gadget and its label after cleaning it\n",
    "                yield clean_gadget(gadget), gadget_val\n",
    "                # Reset for the next gadget\n",
    "                gadget = []\n",
    "\n",
    "            # Check if the line starts with a digit (could be the label or code)\n",
    "            elif stripped.split()[0].isdigit():\n",
    "                if gadget:\n",
    "                    # If it's just a number, treat it as the vulnerability label (e.g., \"1\" or \"0\")\n",
    "                    if stripped.isdigit():\n",
    "                        gadget_val = int(stripped)\n",
    "                    else:\n",
    "                        # Otherwise, it's a code line that happens to start with a number\n",
    "                        gadget.append(stripped)\n",
    "            else:\n",
    "                # Regular code line, add it to the current gadget\n",
    "                gadget.append(stripped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object parse_file at 0x0000024AD7779140>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename = \"cwe119_cgd.txt\"\n",
    "parse_file(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = os.path.splitext(os.path.basename(filename))[0]\n",
    "vector_filename = base + \"_gadget_vectors.pkl\" # Example: 'cwe399_cgd_gadget_vectors.pkl'\n",
    "vector_length = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. VECTORIZE GADGET"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a GadgetVectorizer class, which:\n",
    "\n",
    "- Tokenizes C/C++ code snippets (gadgets).\n",
    "- Trains a Word2Vec model on these tokenized gadgets.\n",
    "- Generates vector representations of gadgets using the trained Word2Vec embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import sys\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "import numpy\n",
    "\n",
    "# =======================\n",
    "# Operator Sets for Tokenization\n",
    "# =======================\n",
    "\n",
    "# Operators with 3 characters \n",
    "operators3 = {'<<=', '>>='}\n",
    "# Operators with 2 characters\n",
    "operators2 = {\n",
    "    '->', '++', '--', \n",
    "    '!~', '<<', '>>', '<=', '>=', \n",
    "    '==', '!=', '&&', '||', '+=', \n",
    "    '-=', '*=', '/=', '%=', '&=', '^=', '|='\n",
    "    }\n",
    "# Operators with 1 character\n",
    "operators1 = { \n",
    "    '(', ')', '[', ']', '.', \n",
    "    '+', '-', '*', '&', '/', \n",
    "    '%', '<', '>', '^', '|', \n",
    "    '=', ',', '?', ':' , ';',\n",
    "    '{', '}'\n",
    "    }\n",
    "\n",
    "\"\"\"\n",
    "A class for tokenizing code gadgets, training a Word2Vec model, and generating\n",
    "fixed-size vector representations of the gadgets.\n",
    "\n",
    "Primary Functions:\n",
    "- Tokenize individual code lines and gadgets.\n",
    "- Buffer gadgets and train Word2Vec embeddings.\n",
    "- Convert tokenized gadgets into 2D vector matrices.\n",
    "\n",
    "Each gadget is treated as a sequence of tokens, and the final vector\n",
    "representation is a matrix of size (50 x vector_length).\n",
    "\"\"\"\n",
    "class GadgetVectorizer:\n",
    "\n",
    "    def __init__(self, vector_length):\n",
    "        \"\"\"\n",
    "        Initialize the GadgetVectorizer.\n",
    "        \"\"\"\n",
    "        self.gadgets = []             # List to store tokenized gadgets for Word2Vec training.\n",
    "        self.vector_length = vector_length  # Dimension of each token vector.\n",
    "        self.forward_slices = 0       # Count of gadgets vectorized in forward direction.\n",
    "        self.backward_slices = 0      # Count of gadgets vectorized in backward direction.\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(line):\n",
    "        \"\"\"\n",
    "        Tokenize a single line of C/C++ code.\n",
    "        Splits the line into tokens including identifiers, keywords, and operators.\n",
    "        Preserves the original order of tokens.\n",
    "        \"\"\"\n",
    "        tmp, w = [], []  # tmp: list of finalized tokens, w: characters building current word\n",
    "        i = 0\n",
    "\n",
    "        while i < len(line):\n",
    "            if line[i] == ' ':\n",
    "                # End of word; finalize and add a space token\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i])\n",
    "                w = []\n",
    "                i += 1\n",
    "            # Check for three-character operators (e.g., <<=)\n",
    "            elif line[i:i+3] in operators3:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i:i+3])\n",
    "                w = []\n",
    "                i += 3\n",
    "            # Check for two-character operators (e.g., ++, ==)\n",
    "            elif line[i:i+2] in operators2:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i:i+2])\n",
    "                w = []\n",
    "                i += 2\n",
    "            # Check for one-character operators (e.g., +, -, ;)\n",
    "            elif line[i] in operators1:\n",
    "                tmp.append(''.join(w))\n",
    "                tmp.append(line[i])\n",
    "                w = []\n",
    "                i += 1\n",
    "            else:\n",
    "                # Part of a word/identifier; collect characters\n",
    "                w.append(line[i])\n",
    "                i += 1\n",
    "\n",
    "        # Filter out empty strings and space tokens\n",
    "        res = list(filter(lambda c: c != '', tmp))\n",
    "        return list(filter(lambda c: c != ' ', res))\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize_gadget(gadget):\n",
    "        \"\"\"\n",
    "        Tokenize an entire gadget (list of code lines).\n",
    "\n",
    "        For each line:\n",
    "            - Tokenize the line.\n",
    "            - Concatenate all tokens into a single list.\n",
    "            - Check if any function calls (tokens starting with 'FUN') exist.\n",
    "        \"\"\"\n",
    "        tokenized = []\n",
    "        function_regex = re.compile(r'FUN(\\d)+')  # Matches tokens like FUN1, FUN2, etc.\n",
    "        backwards_slice = False\n",
    "\n",
    "        for line in gadget:\n",
    "            tokens = GadgetVectorizer.tokenize(line)\n",
    "            tokenized += tokens\n",
    "\n",
    "            # If a function token exists in this line, set backwards_slice to True\n",
    "            if any(function_regex.match(token) for token in tokens):\n",
    "                backwards_slice = True\n",
    "            else:\n",
    "                backwards_slice = False\n",
    "\n",
    "        return tokenized, backwards_slice\n",
    "\n",
    "    def add_gadget(self, gadget):\n",
    "        \"\"\"\n",
    "        Add a tokenized gadget to the training buffer.\n",
    "\n",
    "        Updates forward or backward slice counters depending on the presence of function tokens.\n",
    "        \"\"\"\n",
    "        tokenized_gadget, backwards_slice = GadgetVectorizer.tokenize_gadget(gadget)\n",
    "        self.gadgets.append(tokenized_gadget)\n",
    "        if backwards_slice:\n",
    "            self.backward_slices += 1\n",
    "        else:\n",
    "            self.forward_slices += 1\n",
    "\n",
    "    def vectorize(self, gadget):\n",
    "        \"\"\"\n",
    "        Generate a 2D vector representation of a gadget.\n",
    "\n",
    "        - Tokenizes the gadget.\n",
    "        - Creates a matrix of size (50 x vector_length).\n",
    "        - Fills it with token embeddings from the Word2Vec model.\n",
    "        - Uses forward or backward slicing to populate vectors.\n",
    "        \"\"\"\n",
    "        tokenized_gadget, backwards_slice = GadgetVectorizer.tokenize_gadget(gadget)\n",
    "        vectors = numpy.zeros(shape=(50, self.vector_length))\n",
    "\n",
    "        num_tokens = min(len(tokenized_gadget), 50)\n",
    "\n",
    "        if backwards_slice:\n",
    "            # Populate the matrix from the bottom up (reverse order)\n",
    "            for i in range(num_tokens):\n",
    "                token_index = len(tokenized_gadget) - 1 - i\n",
    "                vectors[49 - i] = self.embeddings[tokenized_gadget[token_index]]\n",
    "        else:\n",
    "            # Populate the matrix from the top down (forward order)\n",
    "            for i in range(num_tokens):\n",
    "                vectors[i] = self.embeddings[tokenized_gadget[i]]\n",
    "\n",
    "        return vectors\n",
    "\n",
    "    def train_model(self):\n",
    "        \"\"\"\n",
    "        Train the Word2Vec model on the buffered tokenized gadgets.\n",
    "\n",
    "        - Uses skip-gram model (`sg=1`) for learning embeddings.\n",
    "        - Sets `min_count=1` to ensure every token has an embedding.\n",
    "        - After training, keeps only the word vectors (embeddings).\n",
    "        - Frees memory by deleting the model and raw gadget data.\n",
    "        \"\"\"\n",
    "        # Train the Word2Vec model on all gadgets\n",
    "        model = Word2Vec(\n",
    "            sentences=self.gadgets,\n",
    "            min_count=1,\n",
    "            vector_size=self.vector_length,\n",
    "            sg=1  # Skip-gram model\n",
    "        )\n",
    "\n",
    "        # Save the learned word vectors\n",
    "        self.embeddings = model.wv\n",
    "\n",
    "        # Clean up to save memory\n",
    "        del model\n",
    "        del self.gadgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Processes a gadget file and returns a DataFrame of vectorized gadgets and their labels.\n",
    "\n",
    "Workflow:\n",
    "---------\n",
    "1. Parse the gadget file using `parse_file()` to extract individual gadgets and their vulnerability labels.\n",
    "2. Store each gadget (code + label) in a list.\n",
    "3. Add each gadget to the `GadgetVectorizer` to build a training corpus.\n",
    "4. After all gadgets are collected, train a Word2Vec model on the tokens.\n",
    "5. Re-iterate over the gadgets and convert each into a fixed-size vector matrix.\n",
    "6. Store each vector and its corresponding label into a list.\n",
    "7. Convert the list into a Pandas DataFrame with two columns: \"vector\" and \"val\".\n",
    "\"\"\"\n",
    "def get_vectors_df(filename, vector_length=100):\n",
    "    gadgets = []  # List to hold all gadgets with their labels\n",
    "    count = 0\n",
    "    vectorizer = GadgetVectorizer(vector_length)  # Initialize vectorizer with embedding dimension\n",
    "\n",
    "    # First pass: parse and collect gadgets, add them to the vectorizer\n",
    "    for gadget, val in parse_file(filename):\n",
    "        count += 1\n",
    "        print(\"Collecting gadgets...\", count, end=\"\\r\")\n",
    "        vectorizer.add_gadget(gadget)  # Tokenize and store gadget for training\n",
    "        row = {\"gadget\": gadget, \"val\": val}  # Store raw gadget + label\n",
    "        gadgets.append(row)\n",
    "\n",
    "    # Print slicing mode stats\n",
    "    print('Found {} forward slices and {} backward slices'\n",
    "          .format(vectorizer.forward_slices, vectorizer.backward_slices))\n",
    "    print()\n",
    "\n",
    "    # Train Word2Vec model on all tokenized gadgets\n",
    "    print(\"Training model...\", end=\"\\r\")\n",
    "    vectorizer.train_model()\n",
    "    print()\n",
    "\n",
    "    vectors = []  # Final list to store vectorized gadgets\n",
    "    count = 0\n",
    "\n",
    "    # Second pass: convert each gadget to a vector\n",
    "    for gadget in gadgets:\n",
    "        count += 1\n",
    "        print(\"Processing gadgets...\", count, end=\"\\r\")\n",
    "        vector = vectorizer.vectorize(gadget[\"gadget\"])  # Get (50 x vector_length) matrix\n",
    "        row = {\"vector\": vector, \"val\": gadget[\"val\"]}  # Store vector + label\n",
    "        vectors.append(row)\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Convert to DataFrame \n",
    "    df = pandas.DataFrame(vectors)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if the preprocessed vector data already exists as a pickle file\n",
    "if os.path.exists(vector_filename):\n",
    "    # Load the DataFrame from the cached pickle file \n",
    "    df = pandas.read_pickle(vector_filename)\n",
    "else:\n",
    "    # If not cached, generate the vectors from the raw gadget file\n",
    "    df = get_vectors_df(filename, vector_length)\n",
    "    # Save the generated DataFrame as a pickle file for future reuse\n",
    "    df.to_pickle(vector_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. BLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import warnings\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM, Bidirectional, LeakyReLU, ReLU\n",
    "from keras.optimizers import Adamax\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.utils import compute_class_weight\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "def set_seed(seed=41, deterministic=True):\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['TF_CUDNN_DETERMINISTIC'] = '1'\n",
    "    if deterministic:\n",
    "        os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "    tf.config.set_visible_devices([], 'GPU')\n",
    "    print(\"[INFO] TensorFlow is running on CPU only.\")\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    tf.config.threading.set_intra_op_parallelism_threads(1)\n",
    "    tf.config.threading.set_inter_op_parallelism_threads(1)\n",
    "    print(f\"[INFO] Reproducibility seed set to {seed}\")\n",
    "\n",
    "\n",
    "class BLSTM:\n",
    "    def __init__(self, data, name=\"blstm_final\", seed=41):\n",
    "        self.seed = seed\n",
    "        self.name = name\n",
    "        set_seed(seed)\n",
    "\n",
    "        vectors = np.stack(data.iloc[:, 0].values)\n",
    "        labels = data.iloc[:, 1].values\n",
    "\n",
    "        # Train/Test split\n",
    "        X_train_raw, X_test, y_train_raw, y_test = train_test_split(\n",
    "            vectors, labels, test_size=0.2, stratify=labels, random_state=seed\n",
    "        )\n",
    "\n",
    "        # Balance training set only\n",
    "        pos_idxs = np.where(y_train_raw == 1)[0]\n",
    "        neg_idxs = np.where(y_train_raw == 0)[0]\n",
    "        rng = np.random.default_rng(seed=seed)\n",
    "        undersampled_neg_idxs = rng.choice(neg_idxs, len(pos_idxs), replace=False)\n",
    "        balanced_idxs = np.concatenate([pos_idxs, undersampled_neg_idxs])\n",
    "\n",
    "        self.X_train = X_train_raw[balanced_idxs]\n",
    "        self.y_train = to_categorical(y_train_raw[balanced_idxs])\n",
    "        self.X_test = X_test\n",
    "        self.y_test = to_categorical(y_test)\n",
    "\n",
    "        # Class weights from unbalanced training data\n",
    "        classes = np.unique(y_train_raw)\n",
    "        weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train_raw)\n",
    "        self.class_weight = dict(zip(classes, weights))\n",
    "\n",
    "        # Static hyperparameters\n",
    "        self.best_params = {\n",
    "            'lstm_units': 256,\n",
    "            'dense_units_1': 256,\n",
    "            'dense_units_2': 256,\n",
    "            'dropout_rate': 0.3219216042267068,\n",
    "            'learning_rate': 0.002628016920729174,\n",
    "            'activation': 'LeakyReLU',\n",
    "            'batch_size': 64,\n",
    "            'epochs': 20\n",
    "        }\n",
    "\n",
    "        self.model = self.build_model_with_params(self.best_params)\n",
    "        self.train_final_model()\n",
    "\n",
    "    def build_model_with_params(self, params):\n",
    "        model = Sequential()\n",
    "        model.add(Bidirectional(LSTM(params['lstm_units']), input_shape=(self.X_train.shape[1], self.X_train.shape[2])))\n",
    "        model.add(Dense(params['dense_units_1']))\n",
    "        model.add(LeakyReLU() if params['activation'] == 'LeakyReLU' else ReLU())\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "        model.add(Dense(params['dense_units_2']))\n",
    "        model.add(LeakyReLU() if params['activation'] == 'LeakyReLU' else ReLU())\n",
    "        model.add(Dropout(params['dropout_rate']))\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "        optimizer = Adamax(learning_rate=params['learning_rate'])\n",
    "        model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def train_final_model(self):\n",
    "        print(f\"[INFO] Training on {len(self.X_train)} samples for {self.best_params['epochs']} epochs...\")\n",
    "        self.model.fit(\n",
    "            self.X_train,\n",
    "            self.y_train,\n",
    "            batch_size=self.best_params['batch_size'],\n",
    "            epochs=self.best_params['epochs'],\n",
    "            class_weight=self.class_weight,\n",
    "            verbose=1\n",
    "        )\n",
    "        self.model.save_weights(self.name + \"_best_model.weights.h5\")\n",
    "\n",
    "    def test(self):\n",
    "        self.model.load_weights(self.name + \"_best_model.weights.h5\")\n",
    "        batch_size = self.best_params['batch_size']\n",
    "        results = self.model.evaluate(self.X_test, self.y_test, batch_size=batch_size, verbose=1)\n",
    "        print(f\"Test Accuracy: {results[1]:.4f}\")\n",
    "\n",
    "        predictions = self.model.predict(self.X_test, batch_size=batch_size).round()\n",
    "        tn, fp, fn, tp = confusion_matrix(\n",
    "            np.argmax(self.y_test, axis=1),\n",
    "            np.argmax(predictions, axis=1)\n",
    "        ).ravel()\n",
    "\n",
    "        print(f'False positive rate: {fp / (fp + tn):.4f}')\n",
    "        print(f'False negative rate: {fn / (fn + tp):.4f}')\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        f1_score = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        print(f'True positive rate (Recall): {recall:.4f}')\n",
    "        print(f'Precision: {precision:.4f}')\n",
    "        print(f'F1 score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] TensorFlow is running on CPU only.\n",
      "[INFO] Reproducibility seed set to 41\n",
      "[INFO] Training on 16704 samples for 20 epochs...\n",
      "Epoch 1/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m44s\u001b[0m 156ms/step - accuracy: 0.5764 - loss: 0.6854\n",
      "Epoch 2/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 159ms/step - accuracy: 0.7046 - loss: 0.5527\n",
      "Epoch 3/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 156ms/step - accuracy: 0.7237 - loss: 0.5294\n",
      "Epoch 4/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 155ms/step - accuracy: 0.7402 - loss: 0.5064\n",
      "Epoch 5/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 155ms/step - accuracy: 0.7510 - loss: 0.4850\n",
      "Epoch 6/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 155ms/step - accuracy: 0.7681 - loss: 0.4550\n",
      "Epoch 7/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 157ms/step - accuracy: 0.7898 - loss: 0.4154\n",
      "Epoch 8/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 157ms/step - accuracy: 0.8122 - loss: 0.3772\n",
      "Epoch 9/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 159ms/step - accuracy: 0.8216 - loss: 0.3576\n",
      "Epoch 10/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 156ms/step - accuracy: 0.8267 - loss: 0.3425\n",
      "Epoch 11/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 157ms/step - accuracy: 0.8356 - loss: 0.3368\n",
      "Epoch 12/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 156ms/step - accuracy: 0.8452 - loss: 0.3206\n",
      "Epoch 13/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 155ms/step - accuracy: 0.8518 - loss: 0.3047\n",
      "Epoch 14/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 155ms/step - accuracy: 0.8573 - loss: 0.2956\n",
      "Epoch 15/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 157ms/step - accuracy: 0.8535 - loss: 0.3123\n",
      "Epoch 16/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 159ms/step - accuracy: 0.8648 - loss: 0.2817\n",
      "Epoch 17/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 156ms/step - accuracy: 0.8659 - loss: 0.2748\n",
      "Epoch 18/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 156ms/step - accuracy: 0.8651 - loss: 0.2741\n",
      "Epoch 19/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 158ms/step - accuracy: 0.8650 - loss: 0.2713\n",
      "Epoch 20/20\n",
      "\u001b[1m261/261\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 156ms/step - accuracy: 0.8632 - loss: 0.2724\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 57ms/step - accuracy: 0.8113 - loss: 0.4156\n",
      "Test Accuracy: 0.8115\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 57ms/step\n",
      "False positive rate: 0.2366\n",
      "False negative rate: 0.0536\n",
      "True positive rate (Recall): 0.9464\n",
      "Precision: 0.5876\n",
      "F1 score: 0.7250\n"
     ]
    }
   ],
   "source": [
    "blstm = BLSTM(df, name=base)\n",
    "blstm.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
